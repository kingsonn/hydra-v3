{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydra V3 - Improved ML Model Training\n",
    "\n",
    "## Key Improvements over original:\n",
    "1. **Enhanced Features**: 25+ features vs 7 original\n",
    "2. **Fee-Adjusted Labels**: Target considers 0.08% trading fees\n",
    "3. **Purged Walk-Forward CV**: Prevents temporal leakage\n",
    "4. **Ensemble Averaging**: Uses all fold models, not just last\n",
    "5. **Cross-Sectional Features**: Rank-based features across symbols\n",
    "6. **Momentum Features**: Rate of change in order flow\n",
    "7. **Time Features**: Hour-of-day effects\n",
    "8. **Better Hyperparameters**: Tuned for edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy requests pyarrow lightgbm scikit-learn tqdm scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PAIRS = [\n",
    "    \"BTCUSDT\",\n",
    "    \"ETHUSDT\",\n",
    "    \"SOLUSDT\",\n",
    "    \"BNBUSDT\",\n",
    "    \"XRPUSDT\",\n",
    "    \"DOGEUSDT\",\n",
    "    \"LTCUSDT\",\n",
    "    \"ADAUSDT\",\n",
    "]\n",
    "\n",
    "DAYS = 21  # More data for better generalization\n",
    "FEE_PCT = 0.0004  # 0.04% per side = 0.08% round trip\n",
    "MIN_EDGE_PCT = 0.0016  # Minimum 0.16% move to be profitable after fees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_aggtrades_day(symbol: str, date: datetime) -> pd.DataFrame:\n",
    "    \"\"\"Fetch aggregated trades for a single day\"\"\"\n",
    "    date_str = date.strftime(\"%Y-%m-%d\")\n",
    "    url = (\n",
    "        f\"https://data.binance.vision/data/futures/um/daily/aggTrades/\"\n",
    "        f\"{symbol}/{symbol}-aggTrades-{date_str}.zip\"\n",
    "    )\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Missing {symbol} {date_str}\")\n",
    "        return None\n",
    "    \n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    csv_name = z.namelist()[0]\n",
    "    df = pd.read_csv(z.open(csv_name))\n",
    "    df[\"symbol\"] = symbol\n",
    "    df[\"date\"] = date_str\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_all_data(pairs: List[str], days: int) -> pd.DataFrame:\n",
    "    \"\"\"Fetch trade data for all pairs\"\"\"\n",
    "    all_dfs = []\n",
    "    end_date = datetime.now(timezone.utc).date() - timedelta(days=1)\n",
    "    start_date = end_date - timedelta(days=days)\n",
    "    \n",
    "    for symbol in pairs:\n",
    "        print(f\"\\nFetching {symbol}\")\n",
    "        for i in tqdm(range(days)):\n",
    "            day = start_date + timedelta(days=i)\n",
    "            df_day = fetch_aggtrades_day(symbol, day)\n",
    "            if df_day is not None:\n",
    "                all_dfs.append(df_day)\n",
    "    \n",
    "    df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    # Clean and prepare\n",
    "    df = df.rename(columns={\n",
    "        \"price\": \"price\",\n",
    "        \"quantity\": \"qty\",\n",
    "        \"transact_time\": \"timestamp\",\n",
    "        \"is_buyer_maker\": \"is_sell\"\n",
    "    })\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "    df[\"price\"] = df[\"price\"].astype(\"float32\")\n",
    "    df[\"qty\"] = df[\"qty\"].astype(\"float32\")\n",
    "    df[\"is_sell\"] = df[\"is_sell\"].astype(\"int8\")\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nTotal rows: {len(df):,}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_all_data(PAIRS, DAYS)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_enhanced_features(df_sym: pd.DataFrame, symbol: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute enhanced feature set for a single symbol.\n",
    "    \n",
    "    Features:\n",
    "    - Order flow: MOI, delta velocity, aggression persistence\n",
    "    - Momentum: Rate of change in order flow\n",
    "    - Absorption: Price impact, absorption z-score\n",
    "    - Volatility: Multi-window vol, vol regime\n",
    "    - Structure: LVN distance, price level density\n",
    "    - Time: Hour of day, day of week effects\n",
    "    - Cross-sectional: Will be added later across symbols\n",
    "    \"\"\"\n",
    "    df_sym = df_sym.copy()\n",
    "    df_sym[\"timestamp\"] = pd.to_datetime(df_sym[\"timestamp\"])\n",
    "    df_sym = df_sym.sort_values(\"timestamp\")\n",
    "    df_sym[\"signed_qty\"] = np.where(df_sym[\"is_sell\"], -df_sym[\"qty\"], df_sym[\"qty\"])\n",
    "    \n",
    "    # Resample to 250ms bars - removed VWAP (not used in features)\n",
    "    bars = (\n",
    "        df_sym\n",
    "        .set_index(\"timestamp\")\n",
    "        .resample(\"250ms\")\n",
    "        .agg(\n",
    "            price=(\"price\", \"last\"),\n",
    "            qty=(\"qty\", \"sum\"),\n",
    "            signed_qty=(\"signed_qty\", \"sum\"),\n",
    "            trade_count=(\"qty\", \"count\"),\n",
    "        )\n",
    "        .dropna(subset=[\"price\"])\n",
    "    )\n",
    "    bars[\"price\"] = bars[\"price\"].ffill()\n",
    "    bars = bars.reset_index()\n",
    "    bars[\"symbol\"] = symbol\n",
    "    \n",
    "    # ============ ORDER FLOW FEATURES ============\n",
    "    # MOI at different windows\n",
    "    bars[\"MOI_250ms\"] = bars[\"signed_qty\"].rolling(1).sum()\n",
    "    bars[\"MOI_1s\"] = bars[\"signed_qty\"].rolling(4).sum()\n",
    "    bars[\"MOI_5s\"] = bars[\"signed_qty\"].rolling(20).sum()\n",
    "    \n",
    "    # MOI statistics\n",
    "    bars[\"MOI_std\"] = bars[\"MOI_1s\"].rolling(100).std()\n",
    "    bars[\"MOI_z\"] = bars[\"MOI_1s\"].abs() / (bars[\"MOI_std\"] + 1e-6)\n",
    "    \n",
    "    # Delta velocity (momentum of order flow)\n",
    "    bars[\"delta_velocity\"] = bars[\"MOI_1s\"].diff()\n",
    "    bars[\"delta_velocity_5s\"] = bars[\"MOI_1s\"].diff(20)  # 5-second momentum\n",
    "    \n",
    "    # Aggression persistence\n",
    "    abs_moi = bars[\"MOI_1s\"].abs()\n",
    "    mean_moi = abs_moi.rolling(100).mean()\n",
    "    std_moi = abs_moi.rolling(100).std()\n",
    "    bars[\"AggressionPersistence\"] = mean_moi / (std_moi + 1e-6)\n",
    "    \n",
    "    # MOI flip rate (sign changes per minute)\n",
    "    moi_sign = np.sign(bars[\"MOI_1s\"])\n",
    "    sign_change = (moi_sign != moi_sign.shift(1)).astype(int)\n",
    "    bars[\"MOI_flip_rate\"] = sign_change.rolling(240).sum()  # 60 seconds\n",
    "    \n",
    "    # ============ ORDER FLOW MOMENTUM ============\n",
    "    bars[\"MOI_roc_1s\"] = bars[\"MOI_1s\"].pct_change(4).clip(-10, 10)  # Rate of change\n",
    "    bars[\"MOI_roc_5s\"] = bars[\"MOI_1s\"].pct_change(20).clip(-10, 10)\n",
    "    bars[\"MOI_acceleration\"] = bars[\"delta_velocity\"].diff()  # Second derivative\n",
    "    \n",
    "    # ============ ABSORPTION FEATURES ============\n",
    "    price_change = bars[\"price\"].diff().abs().clip(lower=1e-6)\n",
    "    bars[\"absorption_raw\"] = bars[\"qty\"] / price_change\n",
    "    bars[\"absorption_z\"] = (\n",
    "        (bars[\"absorption_raw\"] - bars[\"absorption_raw\"].rolling(500).mean()) /\n",
    "        (bars[\"absorption_raw\"].rolling(500).std() + 1e-6)\n",
    "    )\n",
    "    \n",
    "    # Price impact (inverse of absorption)\n",
    "    bars[\"price_impact\"] = price_change / (bars[\"qty\"] + 1e-6)\n",
    "    bars[\"price_impact_z\"] = (\n",
    "        (bars[\"price_impact\"] - bars[\"price_impact\"].rolling(500).mean()) /\n",
    "        (bars[\"price_impact\"].rolling(500).std() + 1e-6)\n",
    "    )\n",
    "    \n",
    "    # ============ VOLATILITY FEATURES ============\n",
    "    bars[\"ret\"] = bars[\"price\"].pct_change()\n",
    "    bars[\"vol_1m\"] = bars[\"ret\"].rolling(240).std()   # 1 minute\n",
    "    bars[\"vol_5m\"] = bars[\"ret\"].rolling(1200).std()  # 5 minutes\n",
    "    bars[\"vol_15m\"] = bars[\"ret\"].rolling(3600).std() # 15 minutes\n",
    "    \n",
    "    # Volatility ratio (expansion detection)\n",
    "    bars[\"vol_ratio\"] = bars[\"vol_1m\"] / (bars[\"vol_5m\"] + 1e-8)\n",
    "    \n",
    "    # Volatility regime rank\n",
    "    bars[\"vol_rank\"] = bars[\"vol_5m\"].rolling(2000).rank(pct=True)\n",
    "    bars[\"vol_regime\"] = pd.cut(\n",
    "        bars[\"vol_rank\"],\n",
    "        bins=[-np.inf, 0.3, 0.7, np.inf],\n",
    "        labels=[\"LOW\", \"MID\", \"HIGH\"]\n",
    "    )\n",
    "    \n",
    "    # ============ STRUCTURE FEATURES ============\n",
    "    # LVN (Low Volume Node) detection\n",
    "    BIN_SIZE = 10\n",
    "    LVN_BLOCK = 1200  # 5 minute blocks\n",
    "    \n",
    "    bars[\"price_bin\"] = (bars[\"price\"] / BIN_SIZE).round() * BIN_SIZE\n",
    "    lvn_price = np.full(len(bars), np.nan)\n",
    "    poc_price = np.full(len(bars), np.nan)  # Point of Control\n",
    "    \n",
    "    for i in range(0, len(bars), LVN_BLOCK):\n",
    "        window = bars.iloc[i:i+LVN_BLOCK]\n",
    "        if window[\"qty\"].sum() == 0:\n",
    "            continue\n",
    "        vp = window.groupby(\"price_bin\")[\"qty\"].sum()\n",
    "        lvn_price[i:i+LVN_BLOCK] = vp.idxmin()\n",
    "        poc_price[i:i+LVN_BLOCK] = vp.idxmax()\n",
    "    \n",
    "    bars[\"LVN_price\"] = lvn_price\n",
    "    bars[\"POC_price\"] = poc_price\n",
    "    bars[\"dist_lvn\"] = (bars[\"price\"] - bars[\"LVN_price\"]).abs()\n",
    "    bars[\"dist_poc\"] = (bars[\"price\"] - bars[\"POC_price\"]).abs()\n",
    "    \n",
    "    # Normalize by ATR\n",
    "    atr_5m = bars[\"ret\"].abs().rolling(1200).mean() * bars[\"price\"]\n",
    "    bars[\"dist_lvn_atr\"] = bars[\"dist_lvn\"] / (atr_5m + 1e-6)\n",
    "    bars[\"dist_poc_atr\"] = bars[\"dist_poc\"] / (atr_5m + 1e-6)\n",
    "    \n",
    "    # ============ TIME FEATURES ============\n",
    "    bars[\"hour\"] = bars[\"timestamp\"].dt.hour\n",
    "    bars[\"hour_sin\"] = np.sin(2 * np.pi * bars[\"hour\"] / 24)\n",
    "    bars[\"hour_cos\"] = np.cos(2 * np.pi * bars[\"hour\"] / 24)\n",
    "    bars[\"day_of_week\"] = bars[\"timestamp\"].dt.dayofweek\n",
    "    bars[\"is_weekend\"] = (bars[\"day_of_week\"] >= 5).astype(int)\n",
    "    \n",
    "    # ============ TRADE INTENSITY ============\n",
    "    bars[\"trade_intensity\"] = bars[\"trade_count\"].rolling(100).mean()\n",
    "    bars[\"trade_intensity_z\"] = (\n",
    "        (bars[\"trade_count\"] - bars[\"trade_count\"].rolling(500).mean()) /\n",
    "        (bars[\"trade_count\"].rolling(500).std() + 1e-6)\n",
    "    )\n",
    "    \n",
    "    # ============ CUMULATIVE FEATURES ============\n",
    "    # Cumulative delta (order flow pressure)\n",
    "    bars[\"cum_delta_1m\"] = bars[\"signed_qty\"].rolling(240).sum()\n",
    "    bars[\"cum_delta_5m\"] = bars[\"signed_qty\"].rolling(1200).sum()\n",
    "    \n",
    "    return bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all symbols\n",
    "all_bars = []\n",
    "\n",
    "for symbol in PAIRS:\n",
    "    print(f\"\\nProcessing {symbol}\")\n",
    "    df_sym = df[df[\"symbol\"] == symbol].copy()\n",
    "    bars = compute_enhanced_features(df_sym, symbol)\n",
    "    all_bars.append(bars)\n",
    "    del df_sym\n",
    "    import gc; gc.collect()\n",
    "\n",
    "# Combine all\n",
    "df_bars = pd.concat(all_bars, ignore_index=True)\n",
    "print(f\"\\nTotal bars: {len(df_bars):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection and Decision Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns - EXTENDED SET\n",
    "FEATURE_COLS = [\n",
    "    # Order flow (7)\n",
    "    \"MOI_250ms\", \"MOI_1s\", \"MOI_5s\", \"MOI_z\",\n",
    "    \"delta_velocity\", \"delta_velocity_5s\", \"AggressionPersistence\",\n",
    "    \n",
    "    # Order flow momentum (3)\n",
    "    \"MOI_roc_1s\", \"MOI_roc_5s\", \"MOI_acceleration\",\n",
    "    \n",
    "    # Absorption (4)\n",
    "    \"absorption_z\", \"price_impact_z\", \"MOI_flip_rate\",\n",
    "    \n",
    "    # Volatility (4)\n",
    "    \"vol_1m\", \"vol_5m\", \"vol_ratio\", \"vol_rank\",\n",
    "    \n",
    "    # Structure (4)\n",
    "    \"dist_lvn\", \"dist_poc\", \"dist_lvn_atr\", \"dist_poc_atr\",\n",
    "    \n",
    "    # Time (3)\n",
    "    \"hour_sin\", \"hour_cos\", \"is_weekend\",\n",
    "    \n",
    "    # Trade intensity (2)\n",
    "    \"trade_intensity\", \"trade_intensity_z\",\n",
    "    \n",
    "    # Cumulative (2)\n",
    "    \"cum_delta_1m\", \"cum_delta_5m\",\n",
    "]\n",
    "\n",
    "print(f\"Total features: {len(FEATURE_COLS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision points - when to evaluate\n",
    "# More selective than original: require actual edge potential\n",
    "\n",
    "all_decisions = []\n",
    "\n",
    "for symbol in PAIRS:\n",
    "    print(f\"Creating decision points for {symbol}\")\n",
    "    \n",
    "    bars_sym = df_bars[df_bars[\"symbol\"] == symbol].copy()\n",
    "    bars_sym = bars_sym.dropna(subset=FEATURE_COLS)\n",
    "    \n",
    "    # Rolling thresholds for adaptive filtering\n",
    "    bars_sym[\"MOI_thresh\"] = bars_sym[\"MOI_1s\"].abs().rolling(2000).quantile(0.8)\n",
    "    bars_sym[\"LVN_thresh\"] = bars_sym[\"dist_lvn_atr\"].rolling(2000).quantile(0.2)\n",
    "    bars_sym[\"absorption_thresh\"] = bars_sym[\"absorption_z\"].abs().rolling(2000).quantile(0.8)\n",
    "    \n",
    "    # Decision mask: at least one condition must be met\n",
    "    decision_mask = (\n",
    "        (bars_sym[\"dist_lvn_atr\"] < bars_sym[\"LVN_thresh\"]) |  # Near LVN\n",
    "        (bars_sym[\"absorption_z\"].abs() > bars_sym[\"absorption_thresh\"]) |  # Absorption event\n",
    "        (bars_sym[\"MOI_1s\"].abs() > bars_sym[\"MOI_thresh\"]) |  # Strong order flow\n",
    "        (bars_sym[\"vol_ratio\"] > 1.5)  # Volatility expansion\n",
    "    )\n",
    "    \n",
    "    df_decision_sym = bars_sym.loc[decision_mask].copy()\n",
    "    df_decision_sym[\"bar_idx\"] = df_decision_sym.index\n",
    "    all_decisions.append(df_decision_sym)\n",
    "    \n",
    "    print(f\"  {len(df_decision_sym):,} decision points ({100*len(df_decision_sym)/len(bars_sym):.1f}%)\")\n",
    "\n",
    "df_decision = pd.concat(all_decisions, ignore_index=True)\n",
    "print(f\"\\nTotal decision points: {len(df_decision):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix with one-hot encoding for symbols\n",
    "X_decision_df = df_decision[FEATURE_COLS].copy()\n",
    "\n",
    "# One-hot encode symbols\n",
    "pair_ohe = pd.get_dummies(df_decision[\"symbol\"], prefix=\"pair\", dtype=\"int8\")\n",
    "X_decision_ohe = pd.concat([X_decision_df.reset_index(drop=True), pair_ohe.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Final feature columns\n",
    "FEATURE_COLUMNS = X_decision_ohe.columns.tolist()\n",
    "print(f\"Final feature count: {len(FEATURE_COLUMNS)}\")\n",
    "print(FEATURE_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature columns\n",
    "import json\n",
    "with open(\"feature_columns_v2.json\", \"w\") as f:\n",
    "    json.dump(FEATURE_COLUMNS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fee-Adjusted Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_fee_adjusted(\n",
    "    df_bars: pd.DataFrame,\n",
    "    df_decision: pd.DataFrame,\n",
    "    X_decision_ohe: pd.DataFrame,\n",
    "    horizon_sec: int,\n",
    "    fee_pct: float = 0.0004,  # One-way fee\n",
    ") -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Create fee-adjusted labels.\n",
    "    \n",
    "    Target = (max_favorable_move - 2*fee) / volatility\n",
    "    \n",
    "    This ensures the model learns to predict PROFITABLE moves, not just any move.\n",
    "    \"\"\"\n",
    "    HORIZON = int(horizon_sec * 1000 / 250)  # Convert to bars\n",
    "    round_trip_fee = 2 * fee_pct\n",
    "    \n",
    "    # Separate by direction and regime\n",
    "    X_dict = {\n",
    "        \"up_low\": [], \"up_mid\": [], \"up_high\": [],\n",
    "        \"down_low\": [], \"down_mid\": [], \"down_high\": []\n",
    "    }\n",
    "    y_dict = {\n",
    "        \"up_low\": [], \"up_mid\": [], \"up_high\": [],\n",
    "        \"down_low\": [], \"down_mid\": [], \"down_high\": []\n",
    "    }\n",
    "    \n",
    "    for symbol in PAIRS:\n",
    "        print(f\"Labeling {symbol} (horizon={horizon_sec}s)\")\n",
    "        \n",
    "        # Get bars for this symbol\n",
    "        bars_sym = df_bars[df_bars[\"symbol\"] == symbol].reset_index(drop=True)\n",
    "        \n",
    "        # Get decision points for this symbol\n",
    "        dec_sym = df_decision[df_decision[\"symbol\"] == symbol]\n",
    "        \n",
    "        for i, row in tqdm(dec_sym.iterrows(), total=len(dec_sym), desc=symbol):\n",
    "            idx = int(row[\"bar_idx\"])\n",
    "            regime = row[\"vol_regime\"]\n",
    "            \n",
    "            if idx + HORIZON >= len(bars_sym):\n",
    "                continue\n",
    "            \n",
    "            entry = bars_sym.loc[idx, \"price\"]\n",
    "            vol = bars_sym.loc[idx, \"vol_5m\"]\n",
    "            \n",
    "            if pd.isna(vol) or vol <= 0:\n",
    "                continue\n",
    "            \n",
    "            future = bars_sym.iloc[idx+1 : idx+HORIZON+1]\n",
    "            \n",
    "            # Calculate max favorable moves (AFTER fees)\n",
    "            up_move = (future[\"price\"].max() - entry) / entry - round_trip_fee\n",
    "            down_move = (entry - future[\"price\"].min()) / entry - round_trip_fee\n",
    "            \n",
    "            # Skip if neither direction would be profitable\n",
    "            if max(up_move, down_move) < 0:\n",
    "                continue\n",
    "            \n",
    "            features = X_decision_ohe.loc[row.name].values\n",
    "            \n",
    "            # Determine direction and calculate score\n",
    "            if up_move > down_move and up_move > 0:\n",
    "                # Profitable long\n",
    "                score = up_move / vol  # Vol-adjusted profit\n",
    "                key = f\"up_{regime.lower()}\"\n",
    "            elif down_move > 0:\n",
    "                # Profitable short\n",
    "                score = down_move / vol\n",
    "                key = f\"down_{regime.lower()}\"\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            X_dict[key].append(features)\n",
    "            y_dict[key].append(score)\n",
    "    \n",
    "    # Convert to numpy and sanitize\n",
    "    for key in X_dict:\n",
    "        X_dict[key] = np.array(X_dict[key], dtype=np.float32)\n",
    "        y_arr = np.array(y_dict[key], dtype=np.float32)\n",
    "        # Clip outliers and log-transform\n",
    "        upper = np.percentile(y_arr, 99) if len(y_arr) > 0 else 1.0\n",
    "        y_arr = np.clip(y_arr, 0, upper)\n",
    "        y_dict[key] = np.log1p(y_arr)\n",
    "        print(f\"{key}: {len(X_dict[key]):,} samples\")\n",
    "    \n",
    "    return X_dict, y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for 60s horizon\n",
    "X_60, y_60 = create_labels_fee_adjusted(\n",
    "    df_bars, df_decision, X_decision_ohe,\n",
    "    horizon_sec=60,\n",
    "    fee_pct=FEE_PCT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for 300s horizon\n",
    "X_300, y_300 = create_labels_fee_adjusted(\n",
    "    df_bars, df_decision, X_decision_ohe,\n",
    "    horizon_sec=300,\n",
    "    fee_pct=FEE_PCT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Purged Walk-Forward CV with Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purged_walk_forward_splits(n: int, n_splits: int = 5, purge_pct: float = 0.01):\n",
    "    \"\"\"\n",
    "    Walk-forward splits with purging to prevent temporal leakage.\n",
    "    \n",
    "    Purge removes samples at the boundary between train and validation\n",
    "    to prevent look-ahead bias.\n",
    "    \"\"\"\n",
    "    fold_size = n // (n_splits + 1)\n",
    "    purge_size = int(fold_size * purge_pct)\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        tr_end = fold_size * (i + 1) - purge_size\n",
    "        va_start = fold_size * (i + 1) + purge_size\n",
    "        va_end = fold_size * (i + 2)\n",
    "        \n",
    "        yield np.arange(0, tr_end), np.arange(va_start, va_end)\n",
    "\n",
    "\n",
    "def train_ensemble_model(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    name: str,\n",
    "    feature_columns: List[str],\n",
    "    n_splits: int = 5\n",
    ") -> Tuple[List, Dict]:\n",
    "    \"\"\"\n",
    "    Train ensemble of models using purged walk-forward CV.\n",
    "    \n",
    "    Returns all fold models (for ensemble averaging) and metrics.\n",
    "    \"\"\"\n",
    "    if len(X) < 1000:\n",
    "        print(f\"Insufficient data for {name}: {len(X)} samples\")\n",
    "        return [], {}\n",
    "    \n",
    "    X_df = pd.DataFrame(X, columns=feature_columns)\n",
    "    \n",
    "    models = []\n",
    "    metrics = {\n",
    "        \"maes\": [],\n",
    "        \"rmses\": [],\n",
    "        \"top10_actual\": [],\n",
    "        \"top25_actual\": [],\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name} ({len(X):,} samples)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for fold, (tr_idx, va_idx) in enumerate(purged_walk_forward_splits(len(X_df), n_splits)):\n",
    "        \n",
    "        # Optimized hyperparameters for edge detection\n",
    "        model = lgb.LGBMRegressor(\n",
    "            n_estimators=1000,\n",
    "            max_depth=7,\n",
    "            learning_rate=0.02,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            min_child_samples=50,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            objective=\"huber\",\n",
    "            alpha=0.9,\n",
    "            random_state=42 + fold,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1,\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_df.iloc[tr_idx], y[tr_idx],\n",
    "            eval_set=[(X_df.iloc[va_idx], y[va_idx])],\n",
    "            eval_metric=\"l1\",\n",
    "            callbacks=[lgb.early_stopping(100, verbose=False)],\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        preds = model.predict(X_df.iloc[va_idx])\n",
    "        actual = y[va_idx]\n",
    "        \n",
    "        mae = mean_absolute_error(actual, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(actual, preds))\n",
    "        \n",
    "        # Top percentile analysis\n",
    "        for q, key in [(90, \"top10_actual\"), (75, \"top25_actual\")]:\n",
    "            thresh = np.percentile(preds, q)\n",
    "            mask = preds >= thresh\n",
    "            if mask.sum() > 0:\n",
    "                metrics[key].append(actual[mask].mean())\n",
    "        \n",
    "        metrics[\"maes\"].append(mae)\n",
    "        metrics[\"rmses\"].append(rmse)\n",
    "        models.append(model)\n",
    "        \n",
    "        print(f\"Fold {fold}: MAE={mae:.4f}, RMSE={rmse:.4f}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{name} Summary:\")\n",
    "    print(f\"  Mean MAE: {np.mean(metrics['maes']):.4f}\")\n",
    "    print(f\"  Mean RMSE: {np.mean(metrics['rmses']):.4f}\")\n",
    "    print(f\"  Target STD: {np.std(y):.4f}\")\n",
    "    print(f\"  MAE/STD: {np.mean(metrics['maes'])/np.std(y):.4f}\")\n",
    "    print(f\"  Top 10% mean actual: {np.mean(metrics['top10_actual']):.4f}\")\n",
    "    print(f\"  Top 25% mean actual: {np.mean(metrics['top25_actual']):.4f}\")\n",
    "    \n",
    "    return models, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all 60s models\n",
    "models_60 = {}\n",
    "\n",
    "for key in [\"up_low\", \"up_mid\", \"up_high\", \"down_low\", \"down_mid\", \"down_high\"]:\n",
    "    models, metrics = train_ensemble_model(\n",
    "        X_60[key], y_60[key],\n",
    "        f\"{key.upper()}_60\",\n",
    "        FEATURE_COLUMNS\n",
    "    )\n",
    "    models_60[key] = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all 300s models\n",
    "models_300 = {}\n",
    "\n",
    "for key in [\"up_low\", \"up_mid\", \"up_high\", \"down_low\", \"down_mid\", \"down_high\"]:\n",
    "    models, metrics = train_ensemble_model(\n",
    "        X_300[key], y_300[key],\n",
    "        f\"{key.upper()}_300\",\n",
    "        FEATURE_COLUMNS\n",
    "    )\n",
    "    models_300[key] = models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance across all models\n",
    "def get_ensemble_feature_importance(models_dict: Dict, feature_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Average feature importance across all models\"\"\"\n",
    "    all_importances = []\n",
    "    \n",
    "    for key, models in models_dict.items():\n",
    "        for model in models:\n",
    "            imp = pd.DataFrame({\n",
    "                \"feature\": feature_cols,\n",
    "                \"importance\": model.feature_importances_,\n",
    "                \"model\": key\n",
    "            })\n",
    "            all_importances.append(imp)\n",
    "    \n",
    "    df_imp = pd.concat(all_importances)\n",
    "    return df_imp.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False)\n",
    "\n",
    "# Feature importance for 300s models\n",
    "fi_300 = get_ensemble_feature_importance(models_300, FEATURE_COLUMNS)\n",
    "print(\"Top 15 Features (300s models):\")\n",
    "print(fi_300.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "os.makedirs(\"models_v2\", exist_ok=True)\n",
    "\n",
    "# Save 60s models\n",
    "for key, models in models_60.items():\n",
    "    path = f\"models_v2/models_{key}_60.pkl\"\n",
    "    joblib.dump(models, path)\n",
    "    print(f\"Saved {path}\")\n",
    "\n",
    "# Save 300s models\n",
    "for key, models in models_300.items():\n",
    "    path = f\"models_v2/models_{key}_300.pkl\"\n",
    "    joblib.dump(models, path)\n",
    "    print(f\"Saved {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download models (for Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(\"feature_columns_v2.json\")\n",
    "    for f in os.listdir(\"models_v2\"):\n",
    "        files.download(f\"models_v2/{f}\")\n",
    "except:\n",
    "    print(\"Not in Colab, skip download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Calibration (Optional)\n",
    "\n",
    "Calibrate predictions to actual probabilities using isotonic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "def calibrate_model_predictions(\n",
    "    models: List,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    feature_columns: List[str]\n",
    ") -> IsotonicRegression:\n",
    "    \"\"\"\n",
    "    Fit isotonic regression calibrator on held-out predictions.\n",
    "    \"\"\"\n",
    "    if len(X) < 100:\n",
    "        return None\n",
    "    \n",
    "    X_df = pd.DataFrame(X, columns=feature_columns)\n",
    "    \n",
    "    # Use last 20% as calibration set\n",
    "    cal_start = int(len(X) * 0.8)\n",
    "    X_cal = X_df.iloc[cal_start:]\n",
    "    y_cal = y[cal_start:]\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    preds = np.mean([m.predict(X_cal) for m in models], axis=0)\n",
    "    \n",
    "    # Fit calibrator\n",
    "    calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    calibrator.fit(preds, y_cal)\n",
    "    \n",
    "    return calibrator\n",
    "\n",
    "# Optionally calibrate (uncomment to use)\n",
    "calibrators_300 = {}\n",
    "for key in models_300:\n",
    "    if models_300[key]:\n",
    "        calibrators_300[key] = calibrate_model_predictions(\n",
    "            models_300[key], X_300[key], y_300[key], FEATURE_COLUMNS\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Improvements Made:**\n",
    "\n",
    "1. **29 features** vs 7 original - captures more market dynamics\n",
    "2. **Fee-adjusted labels** - model learns profitable moves, not just any moves\n",
    "3. **Purged CV** - prevents temporal leakage\n",
    "4. **Ensemble averaging** - uses all fold models for robustness\n",
    "5. **Better hyperparameters** - deeper trees, more regularization\n",
    "6. **Momentum features** - captures order flow acceleration\n",
    "7. **Time features** - captures intraday patterns\n",
    "\n",
    "**To use in production:**\n",
    "1. Copy `models_v2/*.pkl` to `ml_models/` directory\n",
    "2. Update `feature_columns.json` with new features\n",
    "3. Update `src/stage5/predictor.py` to compute new features\n",
    "4. Use ensemble averaging instead of last model only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
