{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydra V3 - Improved ML Model Training\n",
    "\n",
    "## Key Improvements over original:\n",
    "1. **Enhanced Features**: 25+ features vs 7 original\n",
    "2. **Fee-Adjusted Labels**: Target considers 0.08% trading fees\n",
    "3. **Purged Walk-Forward CV**: Prevents temporal leakage\n",
    "4. **Ensemble Averaging**: Uses all fold models, not just last\n",
    "5. **Cross-Sectional Features**: Rank-based features across symbols\n",
    "6. **Momentum Features**: Rate of change in order flow\n",
    "7. **Time Features**: Hour-of-day effects\n",
    "8. **Better Hyperparameters**: Tuned for edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (1.26.3)\n",
      "Requirement already satisfied: requests in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (2.32.3)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (19.0.1)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (4.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (1.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (4.67.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (1.15.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hanso\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy requests pyarrow lightgbm scikit-learn tqdm scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PAIRS = [\n",
    "    \"BTCUSDT\",\n",
    "    \"ETHUSDT\",\n",
    "    \"SOLUSDT\",\n",
    "    \"BNBUSDT\",\n",
    "    \"XRPUSDT\",\n",
    "    \"DOGEUSDT\",\n",
    "    \"LTCUSDT\",\n",
    "    \"ADAUSDT\",\n",
    "]\n",
    "\n",
    "DAYS = 14  # More data for better generalization\n",
    "FEE_PCT = 0.0004  # 0.04% per side = 0.08% round trip\n",
    "MIN_EDGE_PCT = 0.0016  # Minimum 0.16% move to be profitable after fees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_aggtrades_day(symbol: str, date: datetime) -> pd.DataFrame:\n",
    "    \"\"\"Fetch aggregated trades for a single day\"\"\"\n",
    "    date_str = date.strftime(\"%Y-%m-%d\")\n",
    "    url = (\n",
    "        f\"https://data.binance.vision/data/futures/um/daily/aggTrades/\"\n",
    "        f\"{symbol}/{symbol}-aggTrades-{date_str}.zip\"\n",
    "    )\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Missing {symbol} {date_str}\")\n",
    "        return None\n",
    "    \n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    csv_name = z.namelist()[0]\n",
    "    df = pd.read_csv(z.open(csv_name))\n",
    "    df[\"symbol\"] = symbol\n",
    "    df[\"date\"] = date_str\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_all_data(pairs: List[str], days: int) -> pd.DataFrame:\n",
    "    \"\"\"Fetch trade data for all pairs\"\"\"\n",
    "    all_dfs = []\n",
    "    end_date = datetime.now(timezone.utc).date() - timedelta(days=1)\n",
    "    start_date = end_date - timedelta(days=days)\n",
    "    \n",
    "    for symbol in pairs:\n",
    "        print(f\"\\nFetching {symbol}\")\n",
    "        for i in tqdm(range(days)):\n",
    "            day = start_date + timedelta(days=i)\n",
    "            df_day = fetch_aggtrades_day(symbol, day)\n",
    "            if df_day is not None:\n",
    "                all_dfs.append(df_day)\n",
    "    \n",
    "    df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    # Clean and prepare\n",
    "    df = df.rename(columns={\n",
    "        \"price\": \"price\",\n",
    "        \"quantity\": \"qty\",\n",
    "        \"transact_time\": \"timestamp\",\n",
    "        \"is_buyer_maker\": \"is_sell\"\n",
    "    })\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "    df[\"price\"] = df[\"price\"].astype(\"float32\")\n",
    "    df[\"qty\"] = df[\"qty\"].astype(\"float32\")\n",
    "    df[\"is_sell\"] = df[\"is_sell\"].astype(\"int8\")\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nTotal rows: {len(df):,}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching BTCUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:44<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching ETHUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:50<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching SOLUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:24<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching BNBUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:24<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching XRPUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:25<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching DOGEUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:24<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching LTCUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:16<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching ADAUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:20<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows: 58,959,932\n",
      "(58959932, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg_trade_id</th>\n",
       "      <th>price</th>\n",
       "      <th>qty</th>\n",
       "      <th>first_trade_id</th>\n",
       "      <th>last_trade_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>is_sell</th>\n",
       "      <th>symbol</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>983748856</td>\n",
       "      <td>0.117420</td>\n",
       "      <td>830.00</td>\n",
       "      <td>3162451856</td>\n",
       "      <td>3162451861</td>\n",
       "      <td>2026-01-01 00:00:00.033</td>\n",
       "      <td>0</td>\n",
       "      <td>DOGEUSDT</td>\n",
       "      <td>2026-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>983748857</td>\n",
       "      <td>0.117410</td>\n",
       "      <td>51.00</td>\n",
       "      <td>3162451862</td>\n",
       "      <td>3162451862</td>\n",
       "      <td>2026-01-01 00:00:00.740</td>\n",
       "      <td>1</td>\n",
       "      <td>DOGEUSDT</td>\n",
       "      <td>2026-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>712080771</td>\n",
       "      <td>1.841100</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2897077972</td>\n",
       "      <td>2897077972</td>\n",
       "      <td>2026-01-01 00:00:01.589</td>\n",
       "      <td>0</td>\n",
       "      <td>XRPUSDT</td>\n",
       "      <td>2026-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>548850955</td>\n",
       "      <td>0.333300</td>\n",
       "      <td>20.00</td>\n",
       "      <td>1800075962</td>\n",
       "      <td>1800075962</td>\n",
       "      <td>2026-01-01 00:00:01.690</td>\n",
       "      <td>0</td>\n",
       "      <td>ADAUSDT</td>\n",
       "      <td>2026-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1018572200</td>\n",
       "      <td>124.580002</td>\n",
       "      <td>6.43</td>\n",
       "      <td>3047799741</td>\n",
       "      <td>3047799742</td>\n",
       "      <td>2026-01-01 00:00:01.696</td>\n",
       "      <td>1</td>\n",
       "      <td>SOLUSDT</td>\n",
       "      <td>2026-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agg_trade_id       price     qty  first_trade_id  last_trade_id  \\\n",
       "0     983748856    0.117420  830.00      3162451856     3162451861   \n",
       "1     983748857    0.117410   51.00      3162451862     3162451862   \n",
       "2     712080771    1.841100    1.90      2897077972     2897077972   \n",
       "3     548850955    0.333300   20.00      1800075962     1800075962   \n",
       "4    1018572200  124.580002    6.43      3047799741     3047799742   \n",
       "\n",
       "                timestamp  is_sell    symbol        date  \n",
       "0 2026-01-01 00:00:00.033        0  DOGEUSDT  2026-01-01  \n",
       "1 2026-01-01 00:00:00.740        1  DOGEUSDT  2026-01-01  \n",
       "2 2026-01-01 00:00:01.589        0   XRPUSDT  2026-01-01  \n",
       "3 2026-01-01 00:00:01.690        0   ADAUSDT  2026-01-01  \n",
       "4 2026-01-01 00:00:01.696        1   SOLUSDT  2026-01-01  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = fetch_all_data(PAIRS, DAYS)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_enhanced_features(df_sym: pd.DataFrame, symbol: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute enhanced feature set for a single symbol.\n",
    "    \n",
    "    Features:\n",
    "    - Order flow: MOI, delta velocity, aggression persistence\n",
    "    - Momentum: Rate of change in order flow\n",
    "    - Absorption: Price impact, absorption z-score\n",
    "    - Volatility: Multi-window vol, vol regime\n",
    "    - Structure: LVN distance, price level density\n",
    "    - Time: Hour of day, day of week effects\n",
    "    - Cross-sectional: Will be added later across symbols\n",
    "    \"\"\"\n",
    "    df_sym = df_sym.copy()\n",
    "    df_sym[\"timestamp\"] = pd.to_datetime(df_sym[\"timestamp\"])\n",
    "    df_sym = df_sym.sort_values(\"timestamp\")\n",
    "    df_sym[\"signed_qty\"] = np.where(df_sym[\"is_sell\"], -df_sym[\"qty\"], df_sym[\"qty\"])\n",
    "    \n",
    "    # Resample to 250ms bars - removed VWAP (not used in features)\n",
    "    bars = (\n",
    "        df_sym\n",
    "        .set_index(\"timestamp\")\n",
    "        .resample(\"250ms\")\n",
    "        .agg(\n",
    "            price=(\"price\", \"last\"),\n",
    "            qty=(\"qty\", \"sum\"),\n",
    "            signed_qty=(\"signed_qty\", \"sum\"),\n",
    "            trade_count=(\"qty\", \"count\"),\n",
    "        )\n",
    "        .dropna(subset=[\"price\"])\n",
    "    )\n",
    "    bars[\"price\"] = bars[\"price\"].ffill()\n",
    "    bars = bars.reset_index()\n",
    "    bars[\"symbol\"] = symbol\n",
    "    \n",
    "    # ============ ORDER FLOW FEATURES ============\n",
    "    # MOI at different windows\n",
    "    bars[\"MOI_250ms\"] = bars[\"signed_qty\"].rolling(1).sum()\n",
    "    bars[\"MOI_1s\"] = bars[\"signed_qty\"].rolling(4).sum()\n",
    "    bars[\"MOI_5s\"] = bars[\"signed_qty\"].rolling(20).sum()\n",
    "    \n",
    "    # MOI statistics\n",
    "    bars[\"MOI_std\"] = bars[\"MOI_1s\"].rolling(100).std()\n",
    "    bars[\"MOI_z\"] = bars[\"MOI_1s\"].abs() / (bars[\"MOI_std\"] + 1e-6)\n",
    "    \n",
    "    # Delta velocity (momentum of order flow)\n",
    "    bars[\"delta_velocity\"] = bars[\"MOI_1s\"].diff()\n",
    "    bars[\"delta_velocity_5s\"] = bars[\"MOI_1s\"].diff(20)  # 5-second momentum\n",
    "    \n",
    "    # Aggression persistence\n",
    "    abs_moi = bars[\"MOI_1s\"].abs()\n",
    "    mean_moi = abs_moi.rolling(100).mean()\n",
    "    std_moi = abs_moi.rolling(100).std()\n",
    "    bars[\"AggressionPersistence\"] = mean_moi / (std_moi + 1e-6)\n",
    "    \n",
    "    # MOI flip rate (sign changes per minute)\n",
    "    moi_sign = np.sign(bars[\"MOI_1s\"])\n",
    "    sign_change = (moi_sign != moi_sign.shift(1)).astype(int)\n",
    "    bars[\"MOI_flip_rate\"] = sign_change.rolling(240).sum()  # 60 seconds\n",
    "    \n",
    "    # ============ ORDER FLOW MOMENTUM ============\n",
    "    bars[\"MOI_roc_1s\"] = bars[\"MOI_1s\"].pct_change(4).clip(-10, 10)  # Rate of change\n",
    "    bars[\"MOI_roc_5s\"] = bars[\"MOI_1s\"].pct_change(20).clip(-10, 10)\n",
    "    bars[\"MOI_acceleration\"] = bars[\"delta_velocity\"].diff()  # Second derivative\n",
    "    \n",
    "    # ============ ABSORPTION FEATURES ============\n",
    "    price_change = bars[\"price\"].diff().abs().clip(lower=1e-6)\n",
    "    bars[\"absorption_raw\"] = bars[\"qty\"] / price_change\n",
    "    bars[\"absorption_z\"] = (\n",
    "        (bars[\"absorption_raw\"] - bars[\"absorption_raw\"].rolling(500).mean()) /\n",
    "        (bars[\"absorption_raw\"].rolling(500).std() + 1e-6)\n",
    "    )\n",
    "    \n",
    "    # Price impact (inverse of absorption)\n",
    "    bars[\"price_impact\"] = price_change / (bars[\"qty\"] + 1e-6)\n",
    "    bars[\"price_impact_z\"] = (\n",
    "        (bars[\"price_impact\"] - bars[\"price_impact\"].rolling(500).mean()) /\n",
    "        (bars[\"price_impact\"].rolling(500).std() + 1e-6)\n",
    "    )\n",
    "    \n",
    "    # ============ VOLATILITY FEATURES ============\n",
    "    bars[\"ret\"] = bars[\"price\"].pct_change()\n",
    "    bars[\"vol_1m\"] = bars[\"ret\"].rolling(240).std()   # 1 minute\n",
    "    bars[\"vol_5m\"] = bars[\"ret\"].rolling(1200).std()  # 5 minutes\n",
    "    bars[\"vol_15m\"] = bars[\"ret\"].rolling(3600).std() # 15 minutes\n",
    "    \n",
    "    # Volatility ratio (expansion detection)\n",
    "    bars[\"vol_ratio\"] = bars[\"vol_1m\"] / (bars[\"vol_5m\"] + 1e-8)\n",
    "    \n",
    "    # Volatility regime rank\n",
    "    bars[\"vol_rank\"] = bars[\"vol_5m\"].rolling(2000).rank(pct=True)\n",
    "    bars[\"vol_regime\"] = pd.cut(\n",
    "        bars[\"vol_rank\"],\n",
    "        bins=[-np.inf, 0.3, 0.7, np.inf],\n",
    "        labels=[\"LOW\", \"MID\", \"HIGH\"]\n",
    "    )\n",
    "    \n",
    "    # ============ STRUCTURE FEATURES ============\n",
    "    # LVN (Low Volume Node) detection\n",
    "    BIN_SIZE = 10\n",
    "    LVN_BLOCK = 1200  # 5 minute blocks\n",
    "    \n",
    "    bars[\"price_bin\"] = (bars[\"price\"] / BIN_SIZE).round() * BIN_SIZE\n",
    "    lvn_price = np.full(len(bars), np.nan)\n",
    "    poc_price = np.full(len(bars), np.nan)  # Point of Control\n",
    "    \n",
    "    for i in range(0, len(bars), LVN_BLOCK):\n",
    "        window = bars.iloc[i:i+LVN_BLOCK]\n",
    "        if window[\"qty\"].sum() == 0:\n",
    "            continue\n",
    "        vp = window.groupby(\"price_bin\")[\"qty\"].sum()\n",
    "        lvn_price[i:i+LVN_BLOCK] = vp.idxmin()\n",
    "        poc_price[i:i+LVN_BLOCK] = vp.idxmax()\n",
    "    \n",
    "    bars[\"LVN_price\"] = lvn_price\n",
    "    bars[\"POC_price\"] = poc_price\n",
    "    bars[\"dist_lvn\"] = (bars[\"price\"] - bars[\"LVN_price\"]).abs()\n",
    "    bars[\"dist_poc\"] = (bars[\"price\"] - bars[\"POC_price\"]).abs()\n",
    "    \n",
    "    # Normalize by ATR\n",
    "    atr_5m = bars[\"ret\"].abs().rolling(1200).mean() * bars[\"price\"]\n",
    "    bars[\"dist_lvn_atr\"] = bars[\"dist_lvn\"] / (atr_5m + 1e-6)\n",
    "    bars[\"dist_poc_atr\"] = bars[\"dist_poc\"] / (atr_5m + 1e-6)\n",
    "    \n",
    "    # ============ TIME FEATURES ============\n",
    "    bars[\"hour\"] = bars[\"timestamp\"].dt.hour\n",
    "    bars[\"hour_sin\"] = np.sin(2 * np.pi * bars[\"hour\"] / 24)\n",
    "    bars[\"hour_cos\"] = np.cos(2 * np.pi * bars[\"hour\"] / 24)\n",
    "    bars[\"day_of_week\"] = bars[\"timestamp\"].dt.dayofweek\n",
    "    bars[\"is_weekend\"] = (bars[\"day_of_week\"] >= 5).astype(int)\n",
    "    \n",
    "    # ============ TRADE INTENSITY ============\n",
    "    bars[\"trade_intensity\"] = bars[\"trade_count\"].rolling(100).mean()\n",
    "    bars[\"trade_intensity_z\"] = (\n",
    "        (bars[\"trade_count\"] - bars[\"trade_count\"].rolling(500).mean()) /\n",
    "        (bars[\"trade_count\"].rolling(500).std() + 1e-6)\n",
    "    )\n",
    "    \n",
    "    # ============ CUMULATIVE FEATURES ============\n",
    "    # Cumulative delta (order flow pressure)\n",
    "    bars[\"cum_delta_1m\"] = bars[\"signed_qty\"].rolling(240).sum()\n",
    "    bars[\"cum_delta_5m\"] = bars[\"signed_qty\"].rolling(1200).sum()\n",
    "    \n",
    "    return bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing BTCUSDT\n",
      "\n",
      "Processing ETHUSDT\n",
      "\n",
      "Processing SOLUSDT\n",
      "\n",
      "Processing BNBUSDT\n",
      "\n",
      "Processing XRPUSDT\n",
      "\n",
      "Processing DOGEUSDT\n",
      "\n",
      "Processing LTCUSDT\n",
      "\n",
      "Processing ADAUSDT\n",
      "\n",
      "Total bars: 18,464,226\n"
     ]
    }
   ],
   "source": [
    "# Process all symbols\n",
    "all_bars = []\n",
    "\n",
    "for symbol in PAIRS:\n",
    "    print(f\"\\nProcessing {symbol}\")\n",
    "    df_sym = df[df[\"symbol\"] == symbol].copy()\n",
    "    bars = compute_enhanced_features(df_sym, symbol)\n",
    "    all_bars.append(bars)\n",
    "    del df_sym\n",
    "    import gc; gc.collect()\n",
    "\n",
    "# Combine all\n",
    "df_bars = pd.concat(all_bars, ignore_index=True)\n",
    "print(f\"\\nTotal bars: {len(df_bars):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection and Decision Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 28\n"
     ]
    }
   ],
   "source": [
    "# Define feature columns - EXTENDED SET\n",
    "FEATURE_COLS = [\n",
    "    # Order flow (7)\n",
    "    \"MOI_250ms\", \"MOI_1s\", \"MOI_5s\", \"MOI_z\",\n",
    "    \"delta_velocity\", \"delta_velocity_5s\", \"AggressionPersistence\",\n",
    "    \n",
    "    # Order flow momentum (3)\n",
    "    \"MOI_roc_1s\", \"MOI_roc_5s\", \"MOI_acceleration\",\n",
    "    \n",
    "    # Absorption (4)\n",
    "    \"absorption_z\", \"price_impact_z\", \"MOI_flip_rate\",\n",
    "    \n",
    "    # Volatility (4)\n",
    "    \"vol_1m\", \"vol_5m\", \"vol_ratio\", \"vol_rank\",\n",
    "    \n",
    "    # Structure (4)\n",
    "    \"dist_lvn\", \"dist_poc\", \"dist_lvn_atr\", \"dist_poc_atr\",\n",
    "    \n",
    "    # Time (3)\n",
    "    \"hour_sin\", \"hour_cos\", \"is_weekend\",\n",
    "    \n",
    "    # Trade intensity (2)\n",
    "    \"trade_intensity\", \"trade_intensity_z\",\n",
    "    \n",
    "    # Cumulative (2)\n",
    "    \"cum_delta_1m\", \"cum_delta_5m\",\n",
    "]\n",
    "\n",
    "print(f\"Total features: {len(FEATURE_COLS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating decision points for BTCUSDT\n",
      "  1,553,277 decision points (53.0%)\n",
      "Creating decision points for ETHUSDT\n",
      "  1,746,795 decision points (54.2%)\n",
      "Creating decision points for SOLUSDT\n",
      "  1,555,317 decision points (52.6%)\n",
      "Creating decision points for BNBUSDT\n",
      "  1,059,286 decision points (54.1%)\n",
      "Creating decision points for XRPUSDT\n",
      "  1,519,994 decision points (54.5%)\n",
      "Creating decision points for DOGEUSDT\n",
      "  1,140,515 decision points (52.6%)\n",
      "Creating decision points for LTCUSDT\n",
      "  442,036 decision points (53.3%)\n",
      "Creating decision points for ADAUSDT\n",
      "  843,472 decision points (53.2%)\n",
      "\n",
      "Total decision points: 9,860,692\n"
     ]
    }
   ],
   "source": [
    "# Create decision points - when to evaluate\n",
    "# More selective than original: require actual edge potential\n",
    "\n",
    "all_decisions = []\n",
    "\n",
    "for symbol in PAIRS:\n",
    "    print(f\"Creating decision points for {symbol}\")\n",
    "    \n",
    "    bars_sym = df_bars[df_bars[\"symbol\"] == symbol].copy()\n",
    "    bars_sym = bars_sym.dropna(subset=FEATURE_COLS)\n",
    "    \n",
    "    # Rolling thresholds for adaptive filtering\n",
    "    bars_sym[\"MOI_thresh\"] = bars_sym[\"MOI_1s\"].abs().rolling(2000).quantile(0.8)\n",
    "    bars_sym[\"LVN_thresh\"] = bars_sym[\"dist_lvn_atr\"].rolling(2000).quantile(0.2)\n",
    "    bars_sym[\"absorption_thresh\"] = bars_sym[\"absorption_z\"].abs().rolling(2000).quantile(0.8)\n",
    "    \n",
    "    # Decision mask: at least one condition must be met\n",
    "    decision_mask = (\n",
    "        (bars_sym[\"dist_lvn_atr\"] < bars_sym[\"LVN_thresh\"]) |  # Near LVN\n",
    "        (bars_sym[\"absorption_z\"].abs() > bars_sym[\"absorption_thresh\"]) |  # Absorption event\n",
    "        (bars_sym[\"MOI_1s\"].abs() > bars_sym[\"MOI_thresh\"]) |  # Strong order flow\n",
    "        (bars_sym[\"vol_ratio\"] > 1.5)  # Volatility expansion\n",
    "    )\n",
    "    \n",
    "    df_decision_sym = bars_sym.loc[decision_mask].copy()\n",
    "    df_decision_sym[\"bar_idx\"] = df_decision_sym.index\n",
    "    all_decisions.append(df_decision_sym)\n",
    "    \n",
    "    print(f\"  {len(df_decision_sym):,} decision points ({100*len(df_decision_sym)/len(bars_sym):.1f}%)\")\n",
    "\n",
    "df_decision = pd.concat(all_decisions, ignore_index=True)\n",
    "print(f\"\\nTotal decision points: {len(df_decision):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature count: 36\n",
      "['MOI_250ms', 'MOI_1s', 'MOI_5s', 'MOI_z', 'delta_velocity', 'delta_velocity_5s', 'AggressionPersistence', 'MOI_roc_1s', 'MOI_roc_5s', 'MOI_acceleration', 'absorption_z', 'price_impact_z', 'MOI_flip_rate', 'vol_1m', 'vol_5m', 'vol_ratio', 'vol_rank', 'dist_lvn', 'dist_poc', 'dist_lvn_atr', 'dist_poc_atr', 'hour_sin', 'hour_cos', 'is_weekend', 'trade_intensity', 'trade_intensity_z', 'cum_delta_1m', 'cum_delta_5m', 'pair_ADAUSDT', 'pair_BNBUSDT', 'pair_BTCUSDT', 'pair_DOGEUSDT', 'pair_ETHUSDT', 'pair_LTCUSDT', 'pair_SOLUSDT', 'pair_XRPUSDT']\n"
     ]
    }
   ],
   "source": [
    "# Create feature matrix with one-hot encoding for symbols\n",
    "X_decision_df = df_decision[FEATURE_COLS].copy()\n",
    "\n",
    "# One-hot encode symbols\n",
    "pair_ohe = pd.get_dummies(df_decision[\"symbol\"], prefix=\"pair\", dtype=\"int8\")\n",
    "X_decision_ohe = pd.concat([X_decision_df.reset_index(drop=True), pair_ohe.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Final feature columns\n",
    "FEATURE_COLUMNS = X_decision_ohe.columns.tolist()\n",
    "print(f\"Final feature count: {len(FEATURE_COLUMNS)}\")\n",
    "print(FEATURE_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature columns\n",
    "import json\n",
    "with open(\"feature_columns_v2.json\", \"w\") as f:\n",
    "    json.dump(FEATURE_COLUMNS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fee-Adjusted Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_fee_adjusted(\n",
    "    df_bars: pd.DataFrame,\n",
    "    df_decision: pd.DataFrame,\n",
    "    X_decision_ohe: pd.DataFrame,\n",
    "    horizon_sec: int,\n",
    "    fee_pct: float = 0.0004,  # One-way fee\n",
    ") -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Create fee-adjusted labels.\n",
    "    \n",
    "    Target = (max_favorable_move - 2*fee) / volatility\n",
    "    \n",
    "    This ensures the model learns to predict PROFITABLE moves, not just any move.\n",
    "    \"\"\"\n",
    "    HORIZON = int(horizon_sec * 1000 / 250)  # Convert to bars\n",
    "    round_trip_fee = 2 * fee_pct\n",
    "    \n",
    "    # Separate by direction and regime\n",
    "    X_dict = {\n",
    "        \"up_low\": [], \"up_mid\": [], \"up_high\": [],\n",
    "        \"down_low\": [], \"down_mid\": [], \"down_high\": []\n",
    "    }\n",
    "    y_dict = {\n",
    "        \"up_low\": [], \"up_mid\": [], \"up_high\": [],\n",
    "        \"down_low\": [], \"down_mid\": [], \"down_high\": []\n",
    "    }\n",
    "    \n",
    "    for symbol in PAIRS:\n",
    "        print(f\"Labeling {symbol} (horizon={horizon_sec}s)\")\n",
    "        \n",
    "        # Get bars for this symbol\n",
    "        bars_sym = df_bars[df_bars[\"symbol\"] == symbol].reset_index(drop=True)\n",
    "        \n",
    "        # Get decision points for this symbol\n",
    "        dec_sym = df_decision[df_decision[\"symbol\"] == symbol]\n",
    "        \n",
    "        for i, row in tqdm(dec_sym.iterrows(), total=len(dec_sym), desc=symbol):\n",
    "            idx = int(row[\"bar_idx\"])\n",
    "            regime = row[\"vol_regime\"]\n",
    "            \n",
    "            if idx + HORIZON >= len(bars_sym):\n",
    "                continue\n",
    "            \n",
    "            entry = bars_sym.loc[idx, \"price\"]\n",
    "            vol = bars_sym.loc[idx, \"vol_5m\"]\n",
    "            \n",
    "            if pd.isna(vol) or vol <= 0:\n",
    "                continue\n",
    "            \n",
    "            future = bars_sym.iloc[idx+1 : idx+HORIZON+1]\n",
    "            \n",
    "            # Calculate max favorable moves (AFTER fees)\n",
    "            up_move = (future[\"price\"].max() - entry) / entry - round_trip_fee\n",
    "            down_move = (entry - future[\"price\"].min()) / entry - round_trip_fee\n",
    "            \n",
    "            # Skip if neither direction would be profitable\n",
    "            if max(up_move, down_move) < 0:\n",
    "                continue\n",
    "            \n",
    "            features = X_decision_ohe.loc[row.name].values\n",
    "            \n",
    "            # Determine direction and calculate score\n",
    "            if up_move > down_move and up_move > 0:\n",
    "                # Profitable long\n",
    "                score = up_move / vol  # Vol-adjusted profit\n",
    "                key = f\"up_{regime.lower()}\"\n",
    "            elif down_move > 0:\n",
    "                # Profitable short\n",
    "                score = down_move / vol\n",
    "                key = f\"down_{regime.lower()}\"\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            X_dict[key].append(features)\n",
    "            y_dict[key].append(score)\n",
    "    \n",
    "    # Convert to numpy and sanitize\n",
    "    for key in X_dict:\n",
    "        X_dict[key] = np.array(X_dict[key], dtype=np.float32)\n",
    "        y_arr = np.array(y_dict[key], dtype=np.float32)\n",
    "        # Clip outliers and log-transform\n",
    "        upper = np.percentile(y_arr, 99) if len(y_arr) > 0 else 1.0\n",
    "        y_arr = np.clip(y_arr, 0, upper)\n",
    "        y_dict[key] = np.log1p(y_arr)\n",
    "        print(f\"{key}: {len(X_dict[key]):,} samples\")\n",
    "    \n",
    "    return X_dict, y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling BTCUSDT (horizon=60s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BTCUSDT: 100%|██████████| 1553277/1553277 [09:35<00:00, 2701.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling ETHUSDT (horizon=60s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ETHUSDT: 100%|██████████| 1746795/1746795 [02:51<00:00, 10205.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling SOLUSDT (horizon=60s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SOLUSDT: 100%|██████████| 1555317/1555317 [01:45<00:00, 14679.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling BNBUSDT (horizon=60s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BNBUSDT: 100%|██████████| 1059286/1059286 [01:11<00:00, 14828.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling XRPUSDT (horizon=60s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XRPUSDT: 100%|██████████| 1519994/1519994 [01:38<00:00, 15376.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling DOGEUSDT (horizon=60s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DOGEUSDT: 100%|██████████| 1140515/1140515 [01:14<00:00, 15220.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling LTCUSDT (horizon=60s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LTCUSDT: 100%|██████████| 442036/442036 [00:29<00:00, 15186.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling ADAUSDT (horizon=60s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ADAUSDT: 100%|██████████| 843472/843472 [01:12<00:00, 11604.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up_low: 77,762 samples\n",
      "up_mid: 57,942 samples\n",
      "up_high: 104,656 samples\n",
      "down_low: 72,077 samples\n",
      "down_mid: 58,412 samples\n",
      "down_high: 95,177 samples\n"
     ]
    }
   ],
   "source": [
    "# Create labels for 60s horizon\n",
    "X_60, y_60 = create_labels_fee_adjusted(\n",
    "    df_bars, df_decision, X_decision_ohe,\n",
    "    horizon_sec=60,\n",
    "    fee_pct=FEE_PCT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling BTCUSDT (horizon=300s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BTCUSDT: 100%|██████████| 1553277/1553277 [14:06<00:00, 1834.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling ETHUSDT (horizon=300s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ETHUSDT: 100%|██████████| 1746795/1746795 [03:33<00:00, 8186.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling SOLUSDT (horizon=300s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SOLUSDT: 100%|██████████| 1555317/1555317 [02:05<00:00, 12437.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling BNBUSDT (horizon=300s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BNBUSDT: 100%|██████████| 1059286/1059286 [01:25<00:00, 12422.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling XRPUSDT (horizon=300s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XRPUSDT: 100%|██████████| 1519994/1519994 [02:14<00:00, 11287.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling DOGEUSDT (horizon=300s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DOGEUSDT: 100%|██████████| 1140515/1140515 [01:43<00:00, 11024.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling LTCUSDT (horizon=300s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LTCUSDT: 100%|██████████| 442036/442036 [00:33<00:00, 13098.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling ADAUSDT (horizon=300s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ADAUSDT: 100%|██████████| 843472/843472 [01:02<00:00, 13436.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up_low: 229,625 samples\n",
      "up_mid: 167,982 samples\n",
      "up_high: 266,789 samples\n",
      "down_low: 219,728 samples\n",
      "down_mid: 168,772 samples\n",
      "down_high: 246,926 samples\n"
     ]
    }
   ],
   "source": [
    "# Create labels for 300s horizon\n",
    "X_300, y_300 = create_labels_fee_adjusted(\n",
    "    df_bars, df_decision, X_decision_ohe,\n",
    "    horizon_sec=300,\n",
    "    fee_pct=FEE_PCT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Purged Walk-Forward CV with Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purged_walk_forward_splits(n: int, n_splits: int = 5, purge_pct: float = 0.01):\n",
    "    \"\"\"\n",
    "    Walk-forward splits with purging to prevent temporal leakage.\n",
    "    \n",
    "    Purge removes samples at the boundary between train and validation\n",
    "    to prevent look-ahead bias.\n",
    "    \"\"\"\n",
    "    fold_size = n // (n_splits + 1)\n",
    "    purge_size = int(fold_size * purge_pct)\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        tr_end = fold_size * (i + 1) - purge_size\n",
    "        va_start = fold_size * (i + 1) + purge_size\n",
    "        va_end = fold_size * (i + 2)\n",
    "        \n",
    "        yield np.arange(0, tr_end), np.arange(va_start, va_end)\n",
    "\n",
    "\n",
    "def train_ensemble_model(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    name: str,\n",
    "    feature_columns: List[str],\n",
    "    n_splits: int = 5\n",
    ") -> Tuple[List, Dict]:\n",
    "    \"\"\"\n",
    "    Train ensemble of models using purged walk-forward CV.\n",
    "    \n",
    "    Returns all fold models (for ensemble averaging) and metrics.\n",
    "    \"\"\"\n",
    "    if len(X) < 1000:\n",
    "        print(f\"Insufficient data for {name}: {len(X)} samples\")\n",
    "        return [], {}\n",
    "    \n",
    "    X_df = pd.DataFrame(X, columns=feature_columns)\n",
    "    \n",
    "    models = []\n",
    "    metrics = {\n",
    "        \"maes\": [],\n",
    "        \"rmses\": [],\n",
    "        \"top10_actual\": [],\n",
    "        \"top25_actual\": [],\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name} ({len(X):,} samples)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for fold, (tr_idx, va_idx) in enumerate(purged_walk_forward_splits(len(X_df), n_splits)):\n",
    "        \n",
    "        # Optimized hyperparameters for edge detection\n",
    "        model = lgb.LGBMRegressor(\n",
    "            n_estimators=1000,\n",
    "            max_depth=7,\n",
    "            learning_rate=0.02,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            min_child_samples=50,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            objective=\"huber\",\n",
    "            alpha=0.9,\n",
    "            random_state=42 + fold,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1,\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_df.iloc[tr_idx], y[tr_idx],\n",
    "            eval_set=[(X_df.iloc[va_idx], y[va_idx])],\n",
    "            eval_metric=\"l1\",\n",
    "            callbacks=[lgb.early_stopping(100, verbose=False)],\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        preds = model.predict(X_df.iloc[va_idx])\n",
    "        actual = y[va_idx]\n",
    "        \n",
    "        mae = mean_absolute_error(actual, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(actual, preds))\n",
    "        \n",
    "        # Top percentile analysis\n",
    "        for q, key in [(90, \"top10_actual\"), (75, \"top25_actual\")]:\n",
    "            thresh = np.percentile(preds, q)\n",
    "            mask = preds >= thresh\n",
    "            if mask.sum() > 0:\n",
    "                metrics[key].append(actual[mask].mean())\n",
    "        \n",
    "        metrics[\"maes\"].append(mae)\n",
    "        metrics[\"rmses\"].append(rmse)\n",
    "        models.append(model)\n",
    "        \n",
    "        print(f\"Fold {fold}: MAE={mae:.4f}, RMSE={rmse:.4f}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{name} Summary:\")\n",
    "    print(f\"  Mean MAE: {np.mean(metrics['maes']):.4f}\")\n",
    "    print(f\"  Mean RMSE: {np.mean(metrics['rmses']):.4f}\")\n",
    "    print(f\"  Target STD: {np.std(y):.4f}\")\n",
    "    print(f\"  MAE/STD: {np.mean(metrics['maes'])/np.std(y):.4f}\")\n",
    "    print(f\"  Top 10% mean actual: {np.mean(metrics['top10_actual']):.4f}\")\n",
    "    print(f\"  Top 25% mean actual: {np.mean(metrics['top25_actual']):.4f}\")\n",
    "    \n",
    "    return models, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training UP_LOW_60 (77,762 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7962, RMSE=0.9873\n",
      "Fold 1: MAE=0.7386, RMSE=0.9035\n",
      "Fold 2: MAE=0.8004, RMSE=0.9615\n",
      "Fold 3: MAE=0.6937, RMSE=0.8588\n",
      "Fold 4: MAE=0.8301, RMSE=1.0260\n",
      "\n",
      "UP_LOW_60 Summary:\n",
      "  Mean MAE: 0.7718\n",
      "  Mean RMSE: 0.9474\n",
      "  Target STD: 0.9905\n",
      "  MAE/STD: 0.7792\n",
      "  Top 10% mean actual: 2.6922\n",
      "  Top 25% mean actual: 2.5998\n",
      "\n",
      "============================================================\n",
      "Training UP_MID_60 (57,942 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7113, RMSE=0.8947\n",
      "Fold 1: MAE=0.7277, RMSE=0.8809\n",
      "Fold 2: MAE=0.7389, RMSE=0.9071\n",
      "Fold 3: MAE=0.7190, RMSE=0.8839\n",
      "Fold 4: MAE=0.7458, RMSE=0.9175\n",
      "\n",
      "UP_MID_60 Summary:\n",
      "  Mean MAE: 0.7285\n",
      "  Mean RMSE: 0.8968\n",
      "  Target STD: 0.9213\n",
      "  MAE/STD: 0.7908\n",
      "  Top 10% mean actual: 2.4257\n",
      "  Top 25% mean actual: 2.3701\n",
      "\n",
      "============================================================\n",
      "Training UP_HIGH_60 (104,656 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7315, RMSE=0.8999\n",
      "Fold 1: MAE=0.7534, RMSE=0.9284\n",
      "Fold 2: MAE=0.7437, RMSE=0.9369\n",
      "Fold 3: MAE=0.7199, RMSE=0.8957\n",
      "Fold 4: MAE=0.7217, RMSE=0.9047\n",
      "\n",
      "UP_HIGH_60 Summary:\n",
      "  Mean MAE: 0.7340\n",
      "  Mean RMSE: 0.9131\n",
      "  Target STD: 0.9806\n",
      "  MAE/STD: 0.7485\n",
      "  Top 10% mean actual: 2.8567\n",
      "  Top 25% mean actual: 2.6155\n",
      "\n",
      "============================================================\n",
      "Training DOWN_LOW_60 (72,077 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7488, RMSE=0.9236\n",
      "Fold 1: MAE=0.7554, RMSE=0.9291\n",
      "Fold 2: MAE=0.7494, RMSE=0.9282\n",
      "Fold 3: MAE=0.7308, RMSE=0.9084\n",
      "Fold 4: MAE=0.7440, RMSE=0.9241\n",
      "\n",
      "DOWN_LOW_60 Summary:\n",
      "  Mean MAE: 0.7457\n",
      "  Mean RMSE: 0.9227\n",
      "  Target STD: 0.9381\n",
      "  MAE/STD: 0.7948\n",
      "  Top 10% mean actual: 2.6736\n",
      "  Top 25% mean actual: 2.6128\n",
      "\n",
      "============================================================\n",
      "Training DOWN_MID_60 (58,412 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.8117, RMSE=0.9867\n",
      "Fold 1: MAE=0.7364, RMSE=0.9175\n",
      "Fold 2: MAE=0.6881, RMSE=0.8570\n",
      "Fold 3: MAE=0.7946, RMSE=0.9658\n",
      "Fold 4: MAE=0.8020, RMSE=0.9919\n",
      "\n",
      "DOWN_MID_60 Summary:\n",
      "  Mean MAE: 0.7666\n",
      "  Mean RMSE: 0.9438\n",
      "  Target STD: 0.9718\n",
      "  MAE/STD: 0.7888\n",
      "  Top 10% mean actual: 2.6482\n",
      "  Top 25% mean actual: 2.5479\n",
      "\n",
      "============================================================\n",
      "Training DOWN_HIGH_60 (95,177 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7509, RMSE=0.9158\n",
      "Fold 1: MAE=0.7513, RMSE=0.9240\n",
      "Fold 2: MAE=0.7892, RMSE=0.9607\n",
      "Fold 3: MAE=0.7207, RMSE=0.8979\n",
      "Fold 4: MAE=0.7875, RMSE=0.9662\n",
      "\n",
      "DOWN_HIGH_60 Summary:\n",
      "  Mean MAE: 0.7599\n",
      "  Mean RMSE: 0.9329\n",
      "  Target STD: 0.9500\n",
      "  MAE/STD: 0.7999\n",
      "  Top 10% mean actual: 2.6237\n",
      "  Top 25% mean actual: 2.5015\n"
     ]
    }
   ],
   "source": [
    "# Train all 60s models\n",
    "models_60 = {}\n",
    "\n",
    "for key in [\"up_low\", \"up_mid\", \"up_high\", \"down_low\", \"down_mid\", \"down_high\"]:\n",
    "    models, metrics = train_ensemble_model(\n",
    "        X_60[key], y_60[key],\n",
    "        f\"{key.upper()}_60\",\n",
    "        FEATURE_COLUMNS\n",
    "    )\n",
    "    models_60[key] = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training UP_LOW_300 (229,625 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.6131, RMSE=0.8266\n",
      "Fold 1: MAE=0.6571, RMSE=0.8770\n",
      "Fold 2: MAE=0.6681, RMSE=0.8748\n",
      "Fold 3: MAE=0.6897, RMSE=0.9103\n",
      "Fold 4: MAE=0.6881, RMSE=0.8948\n",
      "\n",
      "UP_LOW_300 Summary:\n",
      "  Mean MAE: 0.6632\n",
      "  Mean RMSE: 0.8767\n",
      "  Target STD: 0.9608\n",
      "  MAE/STD: 0.6903\n",
      "  Top 10% mean actual: 4.0259\n",
      "  Top 25% mean actual: 3.7044\n",
      "\n",
      "============================================================\n",
      "Training UP_MID_300 (167,982 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7161, RMSE=0.9206\n",
      "Fold 1: MAE=0.6596, RMSE=0.8598\n",
      "Fold 2: MAE=0.7105, RMSE=0.9151\n",
      "Fold 3: MAE=0.7339, RMSE=0.9375\n",
      "Fold 4: MAE=0.6903, RMSE=0.8505\n",
      "\n",
      "UP_MID_300 Summary:\n",
      "  Mean MAE: 0.7021\n",
      "  Mean RMSE: 0.8967\n",
      "  Target STD: 0.9485\n",
      "  MAE/STD: 0.7402\n",
      "  Top 10% mean actual: 3.8722\n",
      "  Top 25% mean actual: 3.6085\n",
      "\n",
      "============================================================\n",
      "Training UP_HIGH_300 (266,789 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7572, RMSE=0.9638\n",
      "Fold 1: MAE=0.6569, RMSE=0.8592\n",
      "Fold 2: MAE=0.7935, RMSE=1.0116\n",
      "Fold 3: MAE=0.7227, RMSE=0.9259\n",
      "Fold 4: MAE=0.6796, RMSE=0.8548\n",
      "\n",
      "UP_HIGH_300 Summary:\n",
      "  Mean MAE: 0.7220\n",
      "  Mean RMSE: 0.9230\n",
      "  Target STD: 1.0035\n",
      "  MAE/STD: 0.7195\n",
      "  Top 10% mean actual: 3.9096\n",
      "  Top 25% mean actual: 3.5686\n",
      "\n",
      "============================================================\n",
      "Training DOWN_LOW_300 (219,728 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7534, RMSE=0.9646\n",
      "Fold 1: MAE=0.7273, RMSE=0.9320\n",
      "Fold 2: MAE=0.7228, RMSE=0.9406\n",
      "Fold 3: MAE=0.6866, RMSE=0.8851\n",
      "Fold 4: MAE=0.6662, RMSE=0.8490\n",
      "\n",
      "DOWN_LOW_300 Summary:\n",
      "  Mean MAE: 0.7113\n",
      "  Mean RMSE: 0.9143\n",
      "  Target STD: 0.9695\n",
      "  MAE/STD: 0.7337\n",
      "  Top 10% mean actual: 4.0646\n",
      "  Top 25% mean actual: 3.7153\n",
      "\n",
      "============================================================\n",
      "Training DOWN_MID_300 (168,772 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7178, RMSE=0.9090\n",
      "Fold 1: MAE=0.7171, RMSE=0.9020\n",
      "Fold 2: MAE=0.7350, RMSE=0.9381\n",
      "Fold 3: MAE=0.6864, RMSE=0.8838\n",
      "Fold 4: MAE=0.7400, RMSE=0.9294\n",
      "\n",
      "DOWN_MID_300 Summary:\n",
      "  Mean MAE: 0.7193\n",
      "  Mean RMSE: 0.9125\n",
      "  Target STD: 0.9635\n",
      "  MAE/STD: 0.7465\n",
      "  Top 10% mean actual: 3.9176\n",
      "  Top 25% mean actual: 3.5840\n",
      "\n",
      "============================================================\n",
      "Training DOWN_HIGH_300 (246,926 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7668, RMSE=0.9660\n",
      "Fold 1: MAE=0.7231, RMSE=0.9505\n",
      "Fold 2: MAE=0.7697, RMSE=0.9686\n",
      "Fold 3: MAE=0.6923, RMSE=0.9072\n",
      "Fold 4: MAE=0.7375, RMSE=0.9309\n",
      "\n",
      "DOWN_HIGH_300 Summary:\n",
      "  Mean MAE: 0.7379\n",
      "  Mean RMSE: 0.9446\n",
      "  Target STD: 0.9805\n",
      "  MAE/STD: 0.7526\n",
      "  Top 10% mean actual: 3.8693\n",
      "  Top 25% mean actual: 3.4783\n"
     ]
    }
   ],
   "source": [
    "# Train all 300s models\n",
    "models_300 = {}\n",
    "\n",
    "for key in [\"up_low\", \"up_mid\", \"up_high\", \"down_low\", \"down_mid\", \"down_high\"]:\n",
    "    models, metrics = train_ensemble_model(\n",
    "        X_300[key], y_300[key],\n",
    "        f\"{key.upper()}_300\",\n",
    "        FEATURE_COLUMNS\n",
    "    )\n",
    "    models_300[key] = models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Features (300s models):\n",
      "feature\n",
      "vol_5m             494.300000\n",
      "cum_delta_5m       374.100000\n",
      "hour_sin           312.066667\n",
      "hour_cos           283.466667\n",
      "dist_poc           260.133333\n",
      "dist_poc_atr       236.766667\n",
      "dist_lvn           219.233333\n",
      "dist_lvn_atr       194.766667\n",
      "vol_ratio          193.633333\n",
      "vol_1m             168.233333\n",
      "cum_delta_1m       165.633333\n",
      "MOI_flip_rate       79.800000\n",
      "is_weekend          75.300000\n",
      "vol_rank            74.133333\n",
      "trade_intensity     67.466667\n",
      "Name: importance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Analyze feature importance across all models\n",
    "def get_ensemble_feature_importance(models_dict: Dict, feature_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Average feature importance across all models\"\"\"\n",
    "    all_importances = []\n",
    "    \n",
    "    for key, models in models_dict.items():\n",
    "        for model in models:\n",
    "            imp = pd.DataFrame({\n",
    "                \"feature\": feature_cols,\n",
    "                \"importance\": model.feature_importances_,\n",
    "                \"model\": key\n",
    "            })\n",
    "            all_importances.append(imp)\n",
    "    \n",
    "    df_imp = pd.concat(all_importances)\n",
    "    return df_imp.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False)\n",
    "\n",
    "# Feature importance for 300s models\n",
    "fi_300 = get_ensemble_feature_importance(models_300, FEATURE_COLUMNS)\n",
    "print(\"Top 15 Features (300s models):\")\n",
    "print(fi_300.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved models_v2/models_up_low_60.pkl\n",
      "Saved models_v2/models_up_mid_60.pkl\n",
      "Saved models_v2/models_up_high_60.pkl\n",
      "Saved models_v2/models_down_low_60.pkl\n",
      "Saved models_v2/models_down_mid_60.pkl\n",
      "Saved models_v2/models_down_high_60.pkl\n",
      "Saved models_v2/models_up_low_300.pkl\n",
      "Saved models_v2/models_up_mid_300.pkl\n",
      "Saved models_v2/models_up_high_300.pkl\n",
      "Saved models_v2/models_down_low_300.pkl\n",
      "Saved models_v2/models_down_mid_300.pkl\n",
      "Saved models_v2/models_down_high_300.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "os.makedirs(\"models_v2\", exist_ok=True)\n",
    "\n",
    "# Save 60s models\n",
    "for key, models in models_60.items():\n",
    "    path = f\"models_v2/models_{key}_60.pkl\"\n",
    "    joblib.dump(models, path)\n",
    "    print(f\"Saved {path}\")\n",
    "\n",
    "# Save 300s models\n",
    "for key, models in models_300.items():\n",
    "    path = f\"models_v2/models_{key}_300.pkl\"\n",
    "    joblib.dump(models, path)\n",
    "    print(f\"Saved {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in Colab, skip download\n"
     ]
    }
   ],
   "source": [
    "# Download models (for Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(\"feature_columns_v2.json\")\n",
    "    for f in os.listdir(\"models_v2\"):\n",
    "        files.download(f\"models_v2/{f}\")\n",
    "except:\n",
    "    print(\"Not in Colab, skip download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Calibration (Optional)\n",
    "\n",
    "Calibrate predictions to actual probabilities using isotonic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "def calibrate_model_predictions(\n",
    "    models: List,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    feature_columns: List[str]\n",
    ") -> IsotonicRegression:\n",
    "    \"\"\"\n",
    "    Fit isotonic regression calibrator on held-out predictions.\n",
    "    \"\"\"\n",
    "    if len(X) < 100:\n",
    "        return None\n",
    "    \n",
    "    X_df = pd.DataFrame(X, columns=feature_columns)\n",
    "    \n",
    "    # Use last 20% as calibration set\n",
    "    cal_start = int(len(X) * 0.8)\n",
    "    X_cal = X_df.iloc[cal_start:]\n",
    "    y_cal = y[cal_start:]\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    preds = np.mean([m.predict(X_cal) for m in models], axis=0)\n",
    "    \n",
    "    # Fit calibrator\n",
    "    calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    calibrator.fit(preds, y_cal)\n",
    "    \n",
    "    return calibrator\n",
    "\n",
    "# Optionally calibrate (uncomment to use)\n",
    "calibrators_300 = {}\n",
    "for key in models_300:\n",
    "    if models_300[key]:\n",
    "        calibrators_300[key] = calibrate_model_predictions(\n",
    "            models_300[key], X_300[key], y_300[key], FEATURE_COLUMNS\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Improvements Made:**\n",
    "\n",
    "1. **29 features** vs 7 original - captures more market dynamics\n",
    "2. **Fee-adjusted labels** - model learns profitable moves, not just any moves\n",
    "3. **Purged CV** - prevents temporal leakage\n",
    "4. **Ensemble averaging** - uses all fold models for robustness\n",
    "5. **Better hyperparameters** - deeper trees, more regularization\n",
    "6. **Momentum features** - captures order flow acceleration\n",
    "7. **Time features** - captures intraday patterns\n",
    "\n",
    "**To use in production:**\n",
    "1. Copy `models_v2/*.pkl` to `ml_models/` directory\n",
    "2. Update `feature_columns.json` with new features\n",
    "3. Update `src/stage5/predictor.py` to compute new features\n",
    "4. Use ensemble averaging instead of last model only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
