{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydra V3 Enhanced - ML Model Training\n",
    "\n",
    "## Enhancements over V2:\n",
    "1. **Cross-Sectional Features**: Rank-based features across symbols\n",
    "2. **Triple-Barrier Labels**: Realistic TP/SL/Time-based targets\n",
    "3. **Optuna Hyperparameter Optimization**: Per-regime tuning\n",
    "4. **21 Days of Data**: More robust training\n",
    "5. **Memory Optimized**: Chunked processing, float32 throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy requests pyarrow lightgbm scikit-learn tqdm scipy optuna -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import time\n",
    "import gc\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PAIRS = [\n",
    "    \"BTCUSDT\", \"ETHUSDT\", \"SOLUSDT\", \"BNBUSDT\",\n",
    "    \"XRPUSDT\", \"DOGEUSDT\", \"LTCUSDT\", \"ADAUSDT\",\n",
    "]\n",
    "\n",
    "DAYS = 21  # Balanced: more data without memory issues\n",
    "FEE_PCT = 0.0004  # 0.04% per side\n",
    "ROUND_TRIP_FEE = 2 * FEE_PCT  # 0.08%\n",
    "\n",
    "# Triple Barrier Parameters\n",
    "TP_MULT = 2.0  # Take profit at 2x ATR\n",
    "SL_MULT = 1.0  # Stop loss at 1x ATR\n",
    "MAX_HOLDING_BARS = 1200  # 5 minutes max hold (300s / 0.25s per bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Fetching (Memory Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_aggtrades_day(symbol: str, date: datetime) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Fetch aggregated trades for a single day\"\"\"\n",
    "    date_str = date.strftime(\"%Y-%m-%d\")\n",
    "    url = (\n",
    "        f\"https://data.binance.vision/data/futures/um/daily/aggTrades/\"\n",
    "        f\"{symbol}/{symbol}-aggTrades-{date_str}.zip\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        \n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        csv_name = z.namelist()[0]\n",
    "        df = pd.read_csv(z.open(csv_name))\n",
    "        df[\"symbol\"] = symbol\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {symbol} {date_str}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_symbol_data(symbol: str, days: int) -> pd.DataFrame:\n",
    "    \"\"\"Fetch all data for a single symbol\"\"\"\n",
    "    all_dfs = []\n",
    "    end_date = datetime.now(timezone.utc).date() - timedelta(days=1)\n",
    "    start_date = end_date - timedelta(days=days)\n",
    "    \n",
    "    for i in tqdm(range(days), desc=symbol):\n",
    "        day = start_date + timedelta(days=i)\n",
    "        df_day = fetch_aggtrades_day(symbol, day)\n",
    "        if df_day is not None:\n",
    "            all_dfs.append(df_day)\n",
    "    \n",
    "    if not all_dfs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    # Clean and convert to efficient types\n",
    "    df = df.rename(columns={\n",
    "        \"transact_time\": \"timestamp\",\n",
    "        \"is_buyer_maker\": \"is_sell\"\n",
    "    })\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "    df[\"price\"] = df[\"price\"].astype(\"float32\")\n",
    "    df[\"quantity\"] = df[\"quantity\"].astype(\"float32\")\n",
    "    df[\"is_sell\"] = df[\"is_sell\"].astype(\"int8\")\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching BTCUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BTCUSDT: 100%|██████████| 21/21 [01:01<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  24,056,684 trades\n",
      "\n",
      "Fetching ETHUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ETHUSDT: 100%|██████████| 21/21 [01:08<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  27,424,579 trades\n",
      "\n",
      "Fetching SOLUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SOLUSDT: 100%|██████████| 21/21 [00:36<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7,532,866 trades\n",
      "\n",
      "Fetching BNBUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BNBUSDT: 100%|██████████| 21/21 [00:31<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5,728,359 trades\n",
      "\n",
      "Fetching XRPUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XRPUSDT: 100%|██████████| 21/21 [00:36<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6,974,506 trades\n",
      "\n",
      "Fetching DOGEUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DOGEUSDT: 100%|██████████| 21/21 [00:33<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5,315,706 trades\n",
      "\n",
      "Fetching LTCUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LTCUSDT: 100%|██████████| 21/21 [00:23<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1,537,556 trades\n",
      "\n",
      "Fetching ADAUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ADAUSDT: 100%|██████████| 21/21 [00:29<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2,991,577 trades\n",
      "\n",
      "Total trades: 81,561,833\n"
     ]
    }
   ],
   "source": [
    "# Fetch data per symbol to manage memory\n",
    "symbol_data = {}\n",
    "for symbol in PAIRS:\n",
    "    print(f\"\\nFetching {symbol}\")\n",
    "    symbol_data[symbol] = fetch_symbol_data(symbol, DAYS)\n",
    "    print(f\"  {len(symbol_data[symbol]):,} trades\")\n",
    "\n",
    "total_trades = sum(len(df) for df in symbol_data.values())\n",
    "print(f\"\\nTotal trades: {total_trades:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhanced Feature Engineering with Cross-Sectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_base_features(df_sym: pd.DataFrame, symbol: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute base features for a single symbol.\n",
    "    Cross-sectional features added later across all symbols.\n",
    "    \"\"\"\n",
    "    df_sym = df_sym.copy()\n",
    "    df_sym[\"signed_qty\"] = np.where(df_sym[\"is_sell\"], -df_sym[\"quantity\"], df_sym[\"quantity\"])\n",
    "    \n",
    "    # Resample to 250ms bars\n",
    "    bars = (\n",
    "        df_sym\n",
    "        .set_index(\"timestamp\")\n",
    "        .resample(\"250ms\")\n",
    "        .agg(\n",
    "            price=(\"price\", \"last\"),\n",
    "            qty=(\"quantity\", \"sum\"),\n",
    "            signed_qty=(\"signed_qty\", \"sum\"),\n",
    "            trade_count=(\"quantity\", \"count\"),\n",
    "        )\n",
    "        .dropna(subset=[\"price\"])\n",
    "    )\n",
    "    bars[\"price\"] = bars[\"price\"].ffill()\n",
    "    bars = bars.reset_index()\n",
    "    bars[\"symbol\"] = symbol\n",
    "    \n",
    "    # ============ ORDER FLOW FEATURES ============\n",
    "    bars[\"MOI_250ms\"] = bars[\"signed_qty\"].rolling(1).sum()\n",
    "    bars[\"MOI_1s\"] = bars[\"signed_qty\"].rolling(4).sum()\n",
    "    bars[\"MOI_5s\"] = bars[\"signed_qty\"].rolling(20).sum()\n",
    "    bars[\"MOI_std\"] = bars[\"MOI_1s\"].rolling(100).std()\n",
    "    bars[\"MOI_z\"] = bars[\"MOI_1s\"].abs() / (bars[\"MOI_std\"] + 1e-6)\n",
    "    bars[\"delta_velocity\"] = bars[\"MOI_1s\"].diff()\n",
    "    bars[\"delta_velocity_5s\"] = bars[\"MOI_1s\"].diff(20)\n",
    "    \n",
    "    # Aggression persistence\n",
    "    abs_moi = bars[\"MOI_1s\"].abs()\n",
    "    mean_moi = abs_moi.rolling(100).mean()\n",
    "    std_moi = abs_moi.rolling(100).std()\n",
    "    bars[\"AggressionPersistence\"] = mean_moi / (std_moi + 1e-6)\n",
    "    \n",
    "    # MOI flip rate\n",
    "    moi_sign = np.sign(bars[\"MOI_1s\"])\n",
    "    sign_change = (moi_sign != moi_sign.shift(1)).astype(int)\n",
    "    bars[\"MOI_flip_rate\"] = sign_change.rolling(240).sum()\n",
    "    \n",
    "    # Order flow momentum\n",
    "    bars[\"MOI_roc_1s\"] = bars[\"MOI_1s\"].pct_change(4).clip(-10, 10)\n",
    "    bars[\"MOI_roc_5s\"] = bars[\"MOI_1s\"].pct_change(20).clip(-10, 10)\n",
    "    bars[\"MOI_acceleration\"] = bars[\"delta_velocity\"].diff()\n",
    "    \n",
    "    # ============ ABSORPTION FEATURES ============\n",
    "    price_change = bars[\"price\"].diff().abs().clip(lower=1e-6)\n",
    "    bars[\"absorption_raw\"] = bars[\"qty\"] / price_change\n",
    "    bars[\"absorption_z\"] = (\n",
    "        (bars[\"absorption_raw\"] - bars[\"absorption_raw\"].rolling(500).mean()) /\n",
    "        (bars[\"absorption_raw\"].rolling(500).std() + 1e-6)\n",
    "    )\n",
    "    bars[\"price_impact\"] = price_change / (bars[\"qty\"] + 1e-6)\n",
    "    bars[\"price_impact_z\"] = (\n",
    "        (bars[\"price_impact\"] - bars[\"price_impact\"].rolling(500).mean()) /\n",
    "        (bars[\"price_impact\"].rolling(500).std() + 1e-6)\n",
    "    )\n",
    "    \n",
    "    # ============ VOLATILITY FEATURES ============\n",
    "    bars[\"ret\"] = bars[\"price\"].pct_change()\n",
    "    bars[\"vol_1m\"] = bars[\"ret\"].rolling(240).std()\n",
    "    bars[\"vol_5m\"] = bars[\"ret\"].rolling(1200).std()\n",
    "    bars[\"vol_ratio\"] = bars[\"vol_1m\"] / (bars[\"vol_5m\"] + 1e-8)\n",
    "    bars[\"vol_rank\"] = bars[\"vol_5m\"].rolling(2000).rank(pct=True)\n",
    "    \n",
    "    # ATR for triple barrier\n",
    "    bars[\"atr_5m\"] = bars[\"ret\"].abs().rolling(1200).mean() * bars[\"price\"]\n",
    "    \n",
    "    # Vol regime - use STRING type instead of Categorical to survive merges\n",
    "    bars[\"vol_regime\"] = pd.cut(\n",
    "        bars[\"vol_rank\"],\n",
    "        bins=[-np.inf, 0.3, 0.7, np.inf],\n",
    "        labels=[\"LOW\", \"MID\", \"HIGH\"]\n",
    "    ).astype(str)  # Convert to string to survive merge operations\n",
    "    \n",
    "    # ============ STRUCTURE FEATURES ============\n",
    "    BIN_SIZE = 10\n",
    "    LVN_BLOCK = 1200\n",
    "    \n",
    "    bars[\"price_bin\"] = (bars[\"price\"] / BIN_SIZE).round() * BIN_SIZE\n",
    "    lvn_price = np.full(len(bars), np.nan)\n",
    "    poc_price = np.full(len(bars), np.nan)\n",
    "    \n",
    "    for i in range(0, len(bars), LVN_BLOCK):\n",
    "        window = bars.iloc[i:i+LVN_BLOCK]\n",
    "        if window[\"qty\"].sum() == 0:\n",
    "            continue\n",
    "        vp = window.groupby(\"price_bin\")[\"qty\"].sum()\n",
    "        lvn_price[i:i+LVN_BLOCK] = vp.idxmin()\n",
    "        poc_price[i:i+LVN_BLOCK] = vp.idxmax()\n",
    "    \n",
    "    bars[\"LVN_price\"] = lvn_price\n",
    "    bars[\"POC_price\"] = poc_price\n",
    "    bars[\"dist_lvn\"] = (bars[\"price\"] - bars[\"LVN_price\"]).abs()\n",
    "    bars[\"dist_poc\"] = (bars[\"price\"] - bars[\"POC_price\"]).abs()\n",
    "    bars[\"dist_lvn_atr\"] = bars[\"dist_lvn\"] / (bars[\"atr_5m\"] + 1e-6)\n",
    "    bars[\"dist_poc_atr\"] = bars[\"dist_poc\"] / (bars[\"atr_5m\"] + 1e-6)\n",
    "    \n",
    "    # ============ TIME FEATURES ============\n",
    "    bars[\"hour\"] = bars[\"timestamp\"].dt.hour\n",
    "    bars[\"hour_sin\"] = np.sin(2 * np.pi * bars[\"hour\"] / 24)\n",
    "    bars[\"hour_cos\"] = np.cos(2 * np.pi * bars[\"hour\"] / 24)\n",
    "    bars[\"is_weekend\"] = (bars[\"timestamp\"].dt.dayofweek >= 5).astype(int)\n",
    "    \n",
    "    # ============ TRADE INTENSITY ============\n",
    "    bars[\"trade_intensity\"] = bars[\"trade_count\"].rolling(100).mean()\n",
    "    bars[\"trade_intensity_z\"] = (\n",
    "        (bars[\"trade_count\"] - bars[\"trade_count\"].rolling(500).mean()) /\n",
    "        (bars[\"trade_count\"].rolling(500).std() + 1e-6)\n",
    "    )\n",
    "    \n",
    "    # ============ CUMULATIVE FEATURES ============\n",
    "    bars[\"cum_delta_1m\"] = bars[\"signed_qty\"].rolling(240).sum()\n",
    "    bars[\"cum_delta_5m\"] = bars[\"signed_qty\"].rolling(1200).sum()\n",
    "    \n",
    "    # Convert to float32\n",
    "    float_cols = bars.select_dtypes(include=[np.float64]).columns\n",
    "    bars[float_cols] = bars[float_cols].astype(np.float32)\n",
    "    \n",
    "    return bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BTCUSDT\n",
      "  4,277,952 bars\n",
      "Processing ETHUSDT\n",
      "  4,742,141 bars\n",
      "Processing SOLUSDT\n",
      "  4,297,506 bars\n",
      "Processing BNBUSDT\n",
      "  2,816,282 bars\n",
      "Processing XRPUSDT\n",
      "  3,880,707 bars\n",
      "Processing DOGEUSDT\n",
      "  3,067,561 bars\n",
      "Processing LTCUSDT\n",
      "  1,157,837 bars\n",
      "Processing ADAUSDT\n",
      "  2,301,313 bars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process all symbols\n",
    "all_bars = {}\n",
    "\n",
    "for symbol in PAIRS:\n",
    "    print(f\"Processing {symbol}\")\n",
    "    if len(symbol_data[symbol]) > 0:\n",
    "        all_bars[symbol] = compute_base_features(symbol_data[symbol], symbol)\n",
    "        print(f\"  {len(all_bars[symbol]):,} bars\")\n",
    "    \n",
    "    # Free memory\n",
    "    del symbol_data[symbol]\n",
    "    gc.collect()\n",
    "\n",
    "del symbol_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cross_sectional_features(all_bars: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Add cross-sectional features: rank features across all symbols at each timestamp.\n",
    "    \n",
    "    This captures relative strength - which symbol is leading/lagging.\n",
    "    \"\"\"\n",
    "    print(\"Adding cross-sectional features...\")\n",
    "    \n",
    "    # Features to rank across symbols\n",
    "    rank_features = [\"MOI_1s\", \"MOI_5s\", \"vol_5m\", \"absorption_z\", \"cum_delta_5m\"]\n",
    "    \n",
    "    # Get common timestamps (rounded to 250ms)\n",
    "    for symbol, bars in all_bars.items():\n",
    "        bars[\"ts_key\"] = bars[\"timestamp\"].dt.floor(\"250ms\")\n",
    "    \n",
    "    # For each feature, compute rank across symbols\n",
    "    for feature in tqdm(rank_features, desc=\"Cross-sectional\"):\n",
    "        # Build cross-sectional dataframe\n",
    "        cross_df = pd.DataFrame()\n",
    "        for symbol, bars in all_bars.items():\n",
    "            temp = bars[[\"ts_key\", feature]].copy()\n",
    "            temp = temp.rename(columns={feature: symbol})\n",
    "            if cross_df.empty:\n",
    "                cross_df = temp\n",
    "            else:\n",
    "                cross_df = cross_df.merge(temp, on=\"ts_key\", how=\"outer\")\n",
    "        \n",
    "        # Compute rank (0-1) across symbols for each timestamp\n",
    "        symbol_cols = [s for s in PAIRS if s in cross_df.columns]\n",
    "        \n",
    "        # Add rank back to each symbol's bars\n",
    "        for symbol in symbol_cols:\n",
    "            rank_col = f\"{feature}_rank\"\n",
    "            # Get rank for this symbol at each timestamp\n",
    "            symbol_ranks = pd.DataFrame()\n",
    "            symbol_ranks[\"ts_key\"] = cross_df[\"ts_key\"]\n",
    "            symbol_ranks[rank_col] = cross_df[symbol_cols].rank(axis=1, pct=True)[symbol]\n",
    "            \n",
    "            # Merge back\n",
    "            all_bars[symbol] = all_bars[symbol].merge(\n",
    "                symbol_ranks, on=\"ts_key\", how=\"left\"\n",
    "            )\n",
    "            all_bars[symbol][rank_col] = all_bars[symbol][rank_col].fillna(0.5).astype(np.float32)\n",
    "        \n",
    "        del cross_df\n",
    "        gc.collect()\n",
    "    \n",
    "    # Remove ts_key\n",
    "    for symbol in all_bars:\n",
    "        all_bars[symbol] = all_bars[symbol].drop(columns=[\"ts_key\"])\n",
    "    \n",
    "    return all_bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding cross-sectional features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-sectional: 100%|██████████| 5/5 [07:54<00:00, 94.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features per symbol: 49\n"
     ]
    }
   ],
   "source": [
    "all_bars = add_cross_sectional_features(all_bars)\n",
    "print(f\"\\nFeatures per symbol: {len(all_bars[PAIRS[0]].columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection and Decision Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 33\n"
     ]
    }
   ],
   "source": [
    "# Extended feature columns with cross-sectional\n",
    "FEATURE_COLS = [\n",
    "    # Order flow (7)\n",
    "    \"MOI_250ms\", \"MOI_1s\", \"MOI_5s\", \"MOI_z\",\n",
    "    \"delta_velocity\", \"delta_velocity_5s\", \"AggressionPersistence\",\n",
    "    \n",
    "    # Order flow momentum (3)\n",
    "    \"MOI_roc_1s\", \"MOI_roc_5s\", \"MOI_acceleration\",\n",
    "    \n",
    "    # Absorption (3)\n",
    "    \"absorption_z\", \"price_impact_z\", \"MOI_flip_rate\",\n",
    "    \n",
    "    # Volatility (4)\n",
    "    \"vol_1m\", \"vol_5m\", \"vol_ratio\", \"vol_rank\",\n",
    "    \n",
    "    # Structure (4)\n",
    "    \"dist_lvn\", \"dist_poc\", \"dist_lvn_atr\", \"dist_poc_atr\",\n",
    "    \n",
    "    # Time (3)\n",
    "    \"hour_sin\", \"hour_cos\", \"is_weekend\",\n",
    "    \n",
    "    # Trade intensity (2)\n",
    "    \"trade_intensity\", \"trade_intensity_z\",\n",
    "    \n",
    "    # Cumulative (2)\n",
    "    \"cum_delta_1m\", \"cum_delta_5m\",\n",
    "    \n",
    "    # Cross-sectional ranks (5) - NEW\n",
    "    \"MOI_1s_rank\", \"MOI_5s_rank\", \"vol_5m_rank\", \n",
    "    \"absorption_z_rank\", \"cum_delta_5m_rank\",\n",
    "]\n",
    "\n",
    "print(f\"Total features: {len(FEATURE_COLS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating decision points for BTCUSDT\n",
      "  1,789,244 decision points (41.9%)\n",
      "Creating decision points for ETHUSDT\n",
      "  2,079,014 decision points (43.9%)\n",
      "Creating decision points for SOLUSDT\n",
      "  1,892,212 decision points (44.1%)\n",
      "Creating decision points for BNBUSDT\n",
      "  1,250,150 decision points (44.4%)\n",
      "Creating decision points for XRPUSDT\n",
      "  1,796,038 decision points (46.3%)\n",
      "Creating decision points for DOGEUSDT\n",
      "  1,364,778 decision points (44.5%)\n",
      "Creating decision points for LTCUSDT\n",
      "  520,112 decision points (45.0%)\n",
      "Creating decision points for ADAUSDT\n",
      "  1,040,956 decision points (45.3%)\n",
      "\n",
      "Total decision points: 11,732,504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create decision points with stricter filtering\n",
    "all_decisions = []\n",
    "\n",
    "for symbol in PAIRS:\n",
    "    print(f\"Creating decision points for {symbol}\")\n",
    "    \n",
    "    bars_sym = all_bars[symbol].copy()\n",
    "    bars_sym = bars_sym.dropna(subset=FEATURE_COLS)\n",
    "    \n",
    "    # Adaptive thresholds\n",
    "    bars_sym[\"MOI_thresh\"] = bars_sym[\"MOI_1s\"].abs().rolling(2000).quantile(0.85)\n",
    "    bars_sym[\"LVN_thresh\"] = bars_sym[\"dist_lvn_atr\"].rolling(2000).quantile(0.15)\n",
    "    bars_sym[\"absorption_thresh\"] = bars_sym[\"absorption_z\"].abs().rolling(2000).quantile(0.85)\n",
    "    \n",
    "    # Decision mask: require stronger conditions\n",
    "    decision_mask = (\n",
    "        (bars_sym[\"dist_lvn_atr\"] < bars_sym[\"LVN_thresh\"]) |  # Near LVN\n",
    "        (bars_sym[\"absorption_z\"].abs() > bars_sym[\"absorption_thresh\"]) |  # Absorption\n",
    "        (bars_sym[\"MOI_1s\"].abs() > bars_sym[\"MOI_thresh\"]) |  # Strong flow\n",
    "        (bars_sym[\"vol_ratio\"] > 1.8)  # Vol expansion\n",
    "    )\n",
    "    \n",
    "    df_decision_sym = bars_sym.loc[decision_mask].copy()\n",
    "    df_decision_sym[\"bar_idx\"] = df_decision_sym.index\n",
    "    all_decisions.append(df_decision_sym)\n",
    "    \n",
    "    print(f\"  {len(df_decision_sym):,} decision points ({100*len(df_decision_sym)/len(bars_sym):.1f}%)\")\n",
    "\n",
    "df_decision = pd.concat(all_decisions, ignore_index=True)\n",
    "print(f\"\\nTotal decision points: {len(df_decision):,}\")\n",
    "\n",
    "del all_decisions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature count: 41\n",
      "X shape: (11732504, 41), dtype: float32\n",
      "Memory: 1.92 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert features to float32\n",
    "for col in FEATURE_COLS:\n",
    "    if col in df_decision.columns:\n",
    "        df_decision[col] = df_decision[col].astype(np.float32)\n",
    "\n",
    "# One-hot encode symbols\n",
    "pair_ohe = pd.get_dummies(df_decision[\"symbol\"], prefix=\"pair\", dtype=\"int8\")\n",
    "\n",
    "# Final feature columns\n",
    "FEATURE_COLUMNS = FEATURE_COLS + pair_ohe.columns.tolist()\n",
    "print(f\"Final feature count: {len(FEATURE_COLUMNS)}\")\n",
    "\n",
    "# Create X matrix\n",
    "X = np.hstack([\n",
    "    df_decision[FEATURE_COLS].values,\n",
    "    pair_ohe.values.astype(np.float32)\n",
    "])\n",
    "print(f\"X shape: {X.shape}, dtype: {X.dtype}\")\n",
    "print(f\"Memory: {X.nbytes / 1e9:.2f} GB\")\n",
    "\n",
    "del pair_ohe\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature columns\n",
    "with open(\"feature_columns_v3.json\", \"w\") as f:\n",
    "    json.dump(FEATURE_COLUMNS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Triple-Barrier Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ LABELING: Fee-Adjusted Max Favorable Move (IMPROVED) ============\n",
    "# Key fix: Be MORE SELECTIVE about samples to reduce noise\n",
    "\n",
    "def create_labels_fee_adjusted(\n",
    "    all_bars: Dict[str, pd.DataFrame],\n",
    "    df_decision: pd.DataFrame,\n",
    "    X: np.ndarray,\n",
    "    horizon_sec: int,\n",
    "    min_profit_mult: float = 1.5,  # Require move > 1.5x fees to include\n",
    ") -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Create fee-adjusted labels using max favorable move approach.\n",
    "    \n",
    "    Key improvement: Only include samples where profit > min_profit_mult * fees\n",
    "    This filters out marginal cases and focuses on clear opportunities.\n",
    "    \"\"\"\n",
    "    HORIZON = int(horizon_sec * 1000 / 250)  # Convert to bars (250ms each)\n",
    "    MIN_PROFIT = ROUND_TRIP_FEE * min_profit_mult  # e.g., 0.08% * 1.5 = 0.12%\n",
    "    \n",
    "    # Separate by direction and regime\n",
    "    X_dict = {\n",
    "        \"up_low\": [], \"up_mid\": [], \"up_high\": [],\n",
    "        \"down_low\": [], \"down_mid\": [], \"down_high\": []\n",
    "    }\n",
    "    y_dict = {\n",
    "        \"up_low\": [], \"up_mid\": [], \"up_high\": [],\n",
    "        \"down_low\": [], \"down_mid\": [], \"down_high\": []\n",
    "    }\n",
    "    \n",
    "    stats = {\"total\": 0, \"profitable\": 0, \"strong\": 0, \"skipped\": 0}\n",
    "    VALID_REGIMES = {\"LOW\", \"MID\", \"HIGH\"}\n",
    "    \n",
    "    for symbol in PAIRS:\n",
    "        print(f\"Labeling {symbol} (horizon={horizon_sec}s)...\")\n",
    "        \n",
    "        bars_sym = all_bars[symbol]\n",
    "        dec_sym = df_decision[df_decision[\"symbol\"] == symbol].copy()\n",
    "        \n",
    "        if len(dec_sym) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Vectorized labeling for speed\n",
    "        prices = bars_sym[\"price\"].values\n",
    "        vol_5m = bars_sym[\"vol_5m\"].values\n",
    "        n_bars = len(bars_sym)\n",
    "        \n",
    "        # Get decision point data\n",
    "        bar_indices = dec_sym[\"bar_idx\"].values.astype(np.int32)\n",
    "        regimes = dec_sym[\"vol_regime\"].astype(str).str.upper().values\n",
    "        row_names = dec_sym.index.values\n",
    "        \n",
    "        # Pre-allocate results\n",
    "        up_moves = np.full(len(dec_sym), np.nan)\n",
    "        down_moves = np.full(len(dec_sym), np.nan)\n",
    "        vols = np.full(len(dec_sym), np.nan)\n",
    "        \n",
    "        # Vectorized calculation of max favorable moves\n",
    "        for i, idx in enumerate(bar_indices):\n",
    "            if idx + HORIZON >= n_bars:\n",
    "                continue\n",
    "            \n",
    "            entry_price = prices[idx]\n",
    "            vol = vol_5m[idx]\n",
    "            \n",
    "            if np.isnan(vol) or vol <= 0 or np.isnan(entry_price):\n",
    "                continue\n",
    "            \n",
    "            # Future prices\n",
    "            future_prices = prices[idx+1 : idx+HORIZON+1]\n",
    "            \n",
    "            # Max favorable moves (AFTER fees)\n",
    "            up_moves[i] = (future_prices.max() - entry_price) / entry_price - ROUND_TRIP_FEE\n",
    "            down_moves[i] = (entry_price - future_prices.min()) / entry_price - ROUND_TRIP_FEE\n",
    "            vols[i] = vol\n",
    "        \n",
    "        # Process results\n",
    "        valid_mask = ~np.isnan(up_moves) & ~np.isnan(vols)\n",
    "        \n",
    "        for i in np.where(valid_mask)[0]:\n",
    "            regime = regimes[i]\n",
    "            if regime not in VALID_REGIMES:\n",
    "                stats[\"skipped\"] += 1\n",
    "                continue\n",
    "            \n",
    "            up_move = up_moves[i]\n",
    "            down_move = down_moves[i]\n",
    "            vol = vols[i]\n",
    "            best_move = max(up_move, down_move)\n",
    "            \n",
    "            stats[\"total\"] += 1\n",
    "            \n",
    "            # Skip if neither direction is profitable\n",
    "            if best_move < 0:\n",
    "                stats[\"skipped\"] += 1\n",
    "                continue\n",
    "            \n",
    "            stats[\"profitable\"] += 1\n",
    "            \n",
    "            # KEY FIX: Only include STRONG signals (> MIN_PROFIT threshold)\n",
    "            if best_move < MIN_PROFIT:\n",
    "                continue  # Skip marginal cases\n",
    "            \n",
    "            stats[\"strong\"] += 1\n",
    "            \n",
    "            # Normalize by volatility\n",
    "            up_score = up_move / vol if vol > 0 else 0\n",
    "            down_score = down_move / vol if vol > 0 else 0\n",
    "            \n",
    "            # Get features\n",
    "            X_row = X[row_names[i]]\n",
    "            \n",
    "            # Assign to appropriate bucket\n",
    "            regime_lower = regime.lower()\n",
    "            if up_move > down_move:\n",
    "                X_dict[f\"up_{regime_lower}\"].append(X_row)\n",
    "                y_dict[f\"up_{regime_lower}\"].append(up_score)\n",
    "            else:\n",
    "                X_dict[f\"down_{regime_lower}\"].append(X_row)\n",
    "                y_dict[f\"down_{regime_lower}\"].append(down_score)\n",
    "        \n",
    "        print(f\"  {symbol}: {valid_mask.sum():,} valid, {stats['strong']:,} strong so far\")\n",
    "    \n",
    "    # Convert to arrays\n",
    "    X_out = {}\n",
    "    y_out = {}\n",
    "    \n",
    "    for key in X_dict:\n",
    "        if X_dict[key]:\n",
    "            X_out[key] = np.vstack(X_dict[key]).astype(np.float32)\n",
    "            y_arr = np.array(y_dict[key], dtype=np.float32)\n",
    "            y_out[key] = np.log1p(np.clip(y_arr, 0, 100))  # log1p for stability\n",
    "            print(f\"{key}: {len(X_out[key]):,} samples, y_mean={y_arr.mean():.4f}, y_std={y_arr.std():.4f}\")\n",
    "        else:\n",
    "            X_out[key] = np.array([], dtype=np.float32).reshape(0, X.shape[1])\n",
    "            y_out[key] = np.array([], dtype=np.float32)\n",
    "            print(f\"{key}: 0 samples\")\n",
    "    \n",
    "    print(f\"\\nStats: total={stats['total']:,}, profitable={stats['profitable']:,}, strong={stats['strong']:,} ({100*stats['strong']/stats['total']:.1f}%)\")\n",
    "    return X_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Creating 60-second horizon labels\n",
      "============================================================\n",
      "Labeling BTCUSDT (horizon=60s)...\n",
      "  BTCUSDT: 1,789,122 valid, 428,405 profitable so far\n",
      "Labeling ETHUSDT (horizon=60s)...\n",
      "  ETHUSDT: 2,078,774 valid, 1,125,707 profitable so far\n",
      "Labeling SOLUSDT (horizon=60s)...\n",
      "  SOLUSDT: 1,892,027 valid, 2,173,793 profitable so far\n",
      "Labeling BNBUSDT (horizon=60s)...\n",
      "  BNBUSDT: 1,250,097 valid, 2,632,938 profitable so far\n",
      "Labeling XRPUSDT (horizon=60s)...\n",
      "  XRPUSDT: 1,795,798 valid, 3,701,028 profitable so far\n",
      "Labeling DOGEUSDT (horizon=60s)...\n",
      "  DOGEUSDT: 1,364,714 valid, 4,738,632 profitable so far\n",
      "Labeling LTCUSDT (horizon=60s)...\n",
      "  LTCUSDT: 520,079 valid, 5,225,332 profitable so far\n",
      "Labeling ADAUSDT (horizon=60s)...\n",
      "  ADAUSDT: 1,040,813 valid, 6,183,268 profitable so far\n",
      "up_low: 797,579 samples, y_mean=11.5897, y_std=14.8482\n",
      "up_mid: 651,961 samples, y_mean=11.0270, y_std=12.6382\n",
      "up_high: 1,621,959 samples, y_mean=11.1698, y_std=14.3253\n",
      "down_low: 799,539 samples, y_mean=11.4841, y_std=13.3161\n",
      "down_mid: 672,775 samples, y_mean=11.0387, y_std=14.2076\n",
      "down_high: 1,639,455 samples, y_mean=10.5711, y_std=11.6858\n",
      "\n",
      "Stats: {'total': 11731424, 'profitable': 6183268, 'skipped': 5548156}\n",
      "\n",
      "============================================================\n",
      "Creating 300-second horizon labels\n",
      "============================================================\n",
      "Labeling BTCUSDT (horizon=300s)...\n",
      "  BTCUSDT: 1,788,736 valid, 1,285,697 profitable so far\n",
      "Labeling ETHUSDT (horizon=300s)...\n",
      "  ETHUSDT: 2,078,357 valid, 2,976,762 profitable so far\n",
      "Labeling SOLUSDT (horizon=300s)...\n",
      "  SOLUSDT: 1,891,158 valid, 4,790,419 profitable so far\n",
      "Labeling BNBUSDT (horizon=300s)...\n",
      "  BNBUSDT: 1,249,395 valid, 5,898,684 profitable so far\n",
      "Labeling XRPUSDT (horizon=300s)...\n",
      "  XRPUSDT: 1,795,316 valid, 7,594,350 profitable so far\n",
      "Labeling DOGEUSDT (horizon=300s)...\n",
      "  DOGEUSDT: 1,364,450 valid, 8,945,022 profitable so far\n",
      "Labeling LTCUSDT (horizon=300s)...\n",
      "  LTCUSDT: 519,753 valid, 9,464,689 profitable so far\n",
      "Labeling ADAUSDT (horizon=300s)...\n",
      "  ADAUSDT: 1,040,556 valid, 10,505,115 profitable so far\n",
      "up_low: 1,499,611 samples, y_mean=34.5863, y_std=42.5031\n",
      "up_mid: 1,193,875 samples, y_mean=31.5555, y_std=32.8565\n",
      "up_high: 2,577,591 samples, y_mean=30.0105, y_std=35.2678\n",
      "down_low: 1,487,150 samples, y_mean=33.7475, y_std=31.7825\n",
      "down_mid: 1,202,708 samples, y_mean=30.7779, y_std=28.5355\n",
      "down_high: 2,544,180 samples, y_mean=28.0896, y_std=25.5776\n",
      "\n",
      "Stats: {'total': 11727721, 'profitable': 10505115, 'skipped': 1222606}\n"
     ]
    }
   ],
   "source": [
    "# Create labels for BOTH horizons (60s and 300s)\n",
    "# Key: Use stricter filtering to focus on high-quality signals\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Creating 60-second horizon labels (min_profit=1.5x fees)\")\n",
    "print(\"=\" * 60)\n",
    "X_60, y_60 = create_labels_fee_adjusted(all_bars, df_decision, X, horizon_sec=60, min_profit_mult=1.5)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Creating 300-second horizon labels (min_profit=2.0x fees)\")\n",
    "print(\"=\" * 60)\n",
    "# Use stricter filtering for 300s - require stronger moves\n",
    "X_300, y_300 = create_labels_fee_adjusted(all_bars, df_decision, X, horizon_sec=300, min_profit_mult=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUG: vol_regime analysis ===\n",
      "df_decision columns: ['timestamp', 'price', 'qty', 'signed_qty', 'trade_count', 'symbol', 'MOI_250ms', 'MOI_1s', 'MOI_5s', 'MOI_std', 'MOI_z', 'delta_velocity', 'delta_velocity_5s', 'AggressionPersistence', 'MOI_flip_rate', 'MOI_roc_1s', 'MOI_roc_5s', 'MOI_acceleration', 'absorption_raw', 'absorption_z', 'price_impact', 'price_impact_z', 'ret', 'vol_1m', 'vol_5m', 'vol_ratio', 'vol_rank', 'atr_5m', 'vol_regime', 'price_bin', 'LVN_price', 'POC_price', 'dist_lvn', 'dist_poc', 'dist_lvn_atr', 'dist_poc_atr', 'hour', 'hour_sin', 'hour_cos', 'is_weekend', 'trade_intensity', 'trade_intensity_z', 'cum_delta_1m', 'cum_delta_5m', 'MOI_1s_rank', 'MOI_5s_rank', 'vol_5m_rank', 'absorption_z_rank', 'cum_delta_5m_rank', 'MOI_thresh', 'LVN_thresh', 'absorption_thresh', 'bar_idx']\n",
      "\n",
      "vol_regime dtype: object\n",
      "\n",
      "vol_regime unique values: ['MID' 'LOW' 'HIGH']\n",
      "\n",
      "vol_regime value counts:\n",
      "HIGH    5611725\n",
      "LOW     3389545\n",
      "MID     2731234\n",
      "Name: vol_regime, dtype: int64\n",
      "\n",
      "=== all_bars['BTCUSDT'] vol_regime ===\n",
      "dtype: object\n",
      "unique values: ['nan' 'LOW' 'MID' 'HIGH']\n",
      "value counts:\n",
      "LOW     1702871\n",
      "HIGH    1445977\n",
      "MID     1125905\n",
      "nan        3199\n",
      "Name: vol_regime, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Check vol_regime values\n",
    "print(\"=== DEBUG: vol_regime analysis ===\")\n",
    "print(f\"df_decision columns: {list(df_decision.columns)}\")\n",
    "print(f\"\\nvol_regime dtype: {df_decision['vol_regime'].dtype}\")\n",
    "print(f\"\\nvol_regime unique values: {df_decision['vol_regime'].unique()[:20]}\")\n",
    "print(f\"\\nvol_regime value counts:\\n{df_decision['vol_regime'].value_counts(dropna=False).head(10)}\")\n",
    "\n",
    "# Check a sample from all_bars\n",
    "sample_symbol = PAIRS[0]\n",
    "print(f\"\\n=== all_bars['{sample_symbol}'] vol_regime ===\")\n",
    "print(f\"dtype: {all_bars[sample_symbol]['vol_regime'].dtype}\")\n",
    "print(f\"unique values: {all_bars[sample_symbol]['vol_regime'].unique()[:20]}\")\n",
    "print(f\"value counts:\\n{all_bars[sample_symbol]['vol_regime'].value_counts(dropna=False).head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>price</th>\n",
       "      <th>qty</th>\n",
       "      <th>signed_qty</th>\n",
       "      <th>trade_count</th>\n",
       "      <th>symbol</th>\n",
       "      <th>MOI_250ms</th>\n",
       "      <th>MOI_1s</th>\n",
       "      <th>MOI_5s</th>\n",
       "      <th>MOI_std</th>\n",
       "      <th>...</th>\n",
       "      <th>cum_delta_5m</th>\n",
       "      <th>MOI_1s_rank</th>\n",
       "      <th>MOI_5s_rank</th>\n",
       "      <th>vol_5m_rank</th>\n",
       "      <th>absorption_z_rank</th>\n",
       "      <th>cum_delta_5m_rank</th>\n",
       "      <th>MOI_thresh</th>\n",
       "      <th>LVN_thresh</th>\n",
       "      <th>absorption_thresh</th>\n",
       "      <th>bar_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-25 00:46:46.750</td>\n",
       "      <td>87610.500000</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.100</td>\n",
       "      <td>2</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-4.441</td>\n",
       "      <td>1.811990</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.555996</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.2076</td>\n",
       "      <td>52.829445</td>\n",
       "      <td>0.241792</td>\n",
       "      <td>5198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-12-25 00:46:47.250</td>\n",
       "      <td>87610.601562</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-4.434</td>\n",
       "      <td>1.811843</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.523996</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.2076</td>\n",
       "      <td>52.828386</td>\n",
       "      <td>0.241792</td>\n",
       "      <td>5199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-25 00:46:47.500</td>\n",
       "      <td>87610.601562</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-4.363</td>\n",
       "      <td>1.811356</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.286995</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.2076</td>\n",
       "      <td>52.809839</td>\n",
       "      <td>0.241792</td>\n",
       "      <td>5200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-12-25 00:46:47.750</td>\n",
       "      <td>87610.601562</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-4.346</td>\n",
       "      <td>1.810782</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.966995</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.2076</td>\n",
       "      <td>52.732594</td>\n",
       "      <td>0.241792</td>\n",
       "      <td>5201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-25 00:46:48.750</td>\n",
       "      <td>87610.601562</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-4.334</td>\n",
       "      <td>1.810359</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.072996</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.2076</td>\n",
       "      <td>52.697834</td>\n",
       "      <td>0.241792</td>\n",
       "      <td>5202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp         price    qty  signed_qty  trade_count  \\\n",
       "0 2025-12-25 00:46:46.750  87610.500000  0.112       0.100            2   \n",
       "1 2025-12-25 00:46:47.250  87610.601562  0.010       0.010            1   \n",
       "2 2025-12-25 00:46:47.500  87610.601562  0.025       0.025            1   \n",
       "3 2025-12-25 00:46:47.750  87610.601562  0.020       0.020            1   \n",
       "4 2025-12-25 00:46:48.750  87610.601562  0.003       0.003            1   \n",
       "\n",
       "    symbol  MOI_250ms  MOI_1s  MOI_5s   MOI_std  ...  cum_delta_5m  \\\n",
       "0  BTCUSDT      0.100   0.057  -4.441  1.811990  ...    -17.555996   \n",
       "1  BTCUSDT      0.010   0.091  -4.434  1.811843  ...    -17.523996   \n",
       "2  BTCUSDT      0.025   0.137  -4.363  1.811356  ...    -17.286995   \n",
       "3  BTCUSDT      0.020   0.155  -4.346  1.810782  ...    -16.966995   \n",
       "4  BTCUSDT      0.003   0.058  -4.334  1.810359  ...    -17.072996   \n",
       "\n",
       "   MOI_1s_rank  MOI_5s_rank  vol_5m_rank  absorption_z_rank  \\\n",
       "0     1.000000     0.500000     0.500000           0.500000   \n",
       "1     0.666667     0.333333     0.333333           0.333333   \n",
       "2     0.625000     0.250000     0.125000           1.000000   \n",
       "3     0.666667     0.333333     0.333333           0.666667   \n",
       "4     0.800000     0.200000     0.200000           0.800000   \n",
       "\n",
       "   cum_delta_5m_rank  MOI_thresh  LVN_thresh  absorption_thresh  bar_idx  \n",
       "0           1.000000      1.2076   52.829445           0.241792     5198  \n",
       "1           0.666667      1.2076   52.828386           0.241792     5199  \n",
       "2           0.875000      1.2076   52.809839           0.241792     5200  \n",
       "3           0.666667      1.2076   52.732594           0.241792     5201  \n",
       "4           0.800000      1.2076   52.697834           0.241792     5202  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_decision.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>price</th>\n",
       "      <th>qty</th>\n",
       "      <th>signed_qty</th>\n",
       "      <th>trade_count</th>\n",
       "      <th>symbol</th>\n",
       "      <th>MOI_250ms</th>\n",
       "      <th>MOI_1s</th>\n",
       "      <th>MOI_5s</th>\n",
       "      <th>MOI_std</th>\n",
       "      <th>...</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>trade_intensity</th>\n",
       "      <th>trade_intensity_z</th>\n",
       "      <th>cum_delta_1m</th>\n",
       "      <th>cum_delta_5m</th>\n",
       "      <th>MOI_1s_rank</th>\n",
       "      <th>MOI_5s_rank</th>\n",
       "      <th>vol_5m_rank</th>\n",
       "      <th>absorption_z_rank</th>\n",
       "      <th>cum_delta_5m_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-25 00:00:00.000</td>\n",
       "      <td>87627.398438</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.102</td>\n",
       "      <td>1</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>0.102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-12-25 00:00:01.500</td>\n",
       "      <td>87627.296875</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.011</td>\n",
       "      <td>2</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>0.011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-25 00:00:01.750</td>\n",
       "      <td>87627.296875</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.026</td>\n",
       "      <td>4</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>0.026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-12-25 00:00:02.250</td>\n",
       "      <td>87627.398438</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-25 00:00:02.500</td>\n",
       "      <td>87627.398438</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>3</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp         price    qty  signed_qty  trade_count  \\\n",
       "0 2025-12-25 00:00:00.000  87627.398438  0.102       0.102            1   \n",
       "1 2025-12-25 00:00:01.500  87627.296875  0.017       0.011            2   \n",
       "2 2025-12-25 00:00:01.750  87627.296875  0.084       0.026            4   \n",
       "3 2025-12-25 00:00:02.250  87627.398438  0.004       0.004            1   \n",
       "4 2025-12-25 00:00:02.500  87627.398438  0.043      -0.007            3   \n",
       "\n",
       "    symbol  MOI_250ms  MOI_1s  MOI_5s  MOI_std  ...  is_weekend  \\\n",
       "0  BTCUSDT      0.102     NaN     NaN      NaN  ...           0   \n",
       "1  BTCUSDT      0.011     NaN     NaN      NaN  ...           0   \n",
       "2  BTCUSDT      0.026     NaN     NaN      NaN  ...           0   \n",
       "3  BTCUSDT      0.004   0.143     NaN      NaN  ...           0   \n",
       "4  BTCUSDT     -0.007   0.034     NaN      NaN  ...           0   \n",
       "\n",
       "   trade_intensity  trade_intensity_z  cum_delta_1m  cum_delta_5m  \\\n",
       "0              NaN                NaN           NaN           NaN   \n",
       "1              NaN                NaN           NaN           NaN   \n",
       "2              NaN                NaN           NaN           NaN   \n",
       "3              NaN                NaN           NaN           NaN   \n",
       "4              NaN                NaN           NaN           NaN   \n",
       "\n",
       "   MOI_1s_rank  MOI_5s_rank  vol_5m_rank  absorption_z_rank  cum_delta_5m_rank  \n",
       "0     0.500000          0.5          0.5                0.5                0.5  \n",
       "1     0.500000          0.5          0.5                0.5                0.5  \n",
       "2     0.500000          0.5          0.5                0.5                0.5  \n",
       "3     0.500000          0.5          0.5                0.5                0.5  \n",
       "4     0.571429          0.5          0.5                0.5                0.5  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_bars['BTCUSDT'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000e-01,  5.70000e-02, -4.44100e+00, ...,  0.00000e+00,\n",
       "         0.00000e+00,  0.00000e+00],\n",
       "       [ 1.00000e-02,  9.10000e-02, -4.43400e+00, ...,  0.00000e+00,\n",
       "         0.00000e+00,  0.00000e+00],\n",
       "       [ 2.50000e-02,  1.37000e-01, -4.36300e+00, ...,  0.00000e+00,\n",
       "         0.00000e+00,  0.00000e+00],\n",
       "       ...,\n",
       "       [-3.70000e+01, -3.80000e+01,  1.38256e+05, ...,  0.00000e+00,\n",
       "         0.00000e+00,  0.00000e+00],\n",
       "       [-2.00000e+01, -7.00000e+01,  1.38223e+05, ...,  0.00000e+00,\n",
       "         0.00000e+00,  0.00000e+00],\n",
       "       [ 1.05000e+02,  4.70000e+01,  1.38307e+05, ...,  0.00000e+00,\n",
       "         0.00000e+00,  0.00000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling BTCUSDT...\n",
      "  score_up=-17.43bp (TIME), score_down=1.43bp (TIME)\n",
      "  score_up=-17.45bp (TIME), score_down=1.45bp (TIME)\n",
      "  score_up=-17.45bp (TIME), score_down=1.45bp (TIME)\n",
      "  score_up=-17.45bp (TIME), score_down=1.45bp (TIME)\n",
      "  score_up=-17.44bp (TIME), score_down=1.44bp (TIME)\n",
      "  BTCUSDT: 1,789,243 valid samples\n",
      "Labeling ETHUSDT...\n",
      "  score_up=-7.08bp (TIME), score_down=-8.92bp (TIME)\n",
      "  score_up=-7.08bp (TIME), score_down=-8.92bp (TIME)\n",
      "  score_up=-7.08bp (TIME), score_down=-8.92bp (TIME)\n",
      "  score_up=-7.12bp (TIME), score_down=-8.88bp (TIME)\n",
      "  score_up=-6.61bp (TIME), score_down=-9.39bp (TIME)\n",
      "  ETHUSDT: 2,079,013 valid samples\n",
      "Labeling SOLUSDT...\n",
      "  score_up=-20.27bp (TIME), score_down=4.27bp (TIME)\n",
      "  score_up=-20.27bp (TIME), score_down=4.27bp (TIME)\n",
      "  score_up=-19.46bp (TIME), score_down=3.46bp (TIME)\n",
      "  score_up=-18.64bp (TIME), score_down=2.64bp (TIME)\n",
      "  score_up=-19.46bp (TIME), score_down=3.46bp (TIME)\n",
      "  SOLUSDT: 1,892,212 valid samples\n",
      "Labeling BNBUSDT...\n",
      "  score_up=-8.12bp (TIME), score_down=-7.88bp (TIME)\n",
      "  score_up=-8.12bp (TIME), score_down=-7.88bp (TIME)\n",
      "  score_up=-8.47bp (TIME), score_down=-7.53bp (TIME)\n",
      "  score_up=-8.71bp (TIME), score_down=-7.29bp (TIME)\n",
      "  score_up=-9.06bp (TIME), score_down=-6.94bp (TIME)\n",
      "  BNBUSDT: 1,250,150 valid samples\n",
      "Labeling XRPUSDT...\n",
      "  score_up=-29.50bp (SL), score_down=9.20bp (TIME)\n",
      "  score_up=-29.50bp (SL), score_down=7.59bp (TIME)\n",
      "  score_up=-29.50bp (SL), score_down=7.05bp (TIME)\n",
      "  score_up=-28.42bp (SL), score_down=10.27bp (TIME)\n",
      "  score_up=-28.42bp (SL), score_down=10.27bp (TIME)\n",
      "  XRPUSDT: 1,796,037 valid samples\n",
      "Labeling DOGEUSDT...\n",
      "  score_up=-28.26bp (SL), score_down=-15.79bp (TIME)\n",
      "  score_up=-28.26bp (SL), score_down=-16.57bp (TIME)\n",
      "  score_up=-28.26bp (SL), score_down=-16.57bp (TIME)\n",
      "  score_up=-28.26bp (SL), score_down=-18.13bp (TIME)\n",
      "  score_up=-28.26bp (SL), score_down=-22.03bp (TIME)\n",
      "  DOGEUSDT: 1,364,778 valid samples\n",
      "Labeling LTCUSDT...\n",
      "  score_up=-28.99bp (SL), score_down=-23.74bp (TIME)\n",
      "  score_up=-28.99bp (SL), score_down=-22.43bp (TIME)\n",
      "  score_up=-28.99bp (SL), score_down=-22.43bp (TIME)\n",
      "  score_up=-28.99bp (SL), score_down=-22.43bp (TIME)\n",
      "  score_up=-28.99bp (SL), score_down=-23.74bp (TIME)\n",
      "  LTCUSDT: 520,112 valid samples\n",
      "Labeling ADAUSDT...\n",
      "  score_up=34.11bp (TP), score_down=-30.46bp (SL)\n",
      "  score_up=34.11bp (TP), score_down=-30.46bp (SL)\n",
      "  score_up=34.11bp (TP), score_down=-30.46bp (SL)\n",
      "  score_up=34.10bp (TP), score_down=-30.45bp (SL)\n",
      "  score_up=0.42bp (TIME), score_down=-30.45bp (SL)\n",
      "  ADAUSDT: 1,040,955 valid samples\n",
      "up_low: 1,695,505 samples, y_mean=8.30bp, y>0: 963,227\n",
      "up_mid: 1,355,831 samples, y_mean=8.44bp, y>0: 772,403\n",
      "up_high: 2,789,828 samples, y_mean=10.26bp, y>0: 1,685,237\n",
      "down_low: 1,694,040 samples, y_mean=8.13bp, y>0: 949,964\n",
      "down_mid: 1,375,401 samples, y_mean=8.24bp, y>0: 757,605\n",
      "down_high: 2,821,895 samples, y_mean=9.72bp, y>0: 1,613,322\n",
      "\n",
      "Exit stats: {'TP': 1887978, 'SL': 428649, 'TIME': 9415873, 'SKIP': 4}\n",
      "Score stats: total=11732500, positive=6741758 (57.5%), avg=6.11bp\n"
     ]
    }
   ],
   "source": [
    "X_data, y_data = create_triple_barrier_labels(all_bars, df_decision, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optuna Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purged_walk_forward_splits(n: int, n_splits: int = 5, purge_pct: float = 0.01):\n",
    "    \"\"\"Walk-forward splits with purging\"\"\"\n",
    "    fold_size = n // (n_splits + 1)\n",
    "    purge_size = int(fold_size * purge_pct)\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        tr_end = fold_size * (i + 1) - purge_size\n",
    "        va_start = fold_size * (i + 1) + purge_size\n",
    "        va_end = fold_size * (i + 2)\n",
    "        \n",
    "        yield np.arange(0, tr_end), np.arange(va_start, va_end)\n",
    "\n",
    "\n",
    "def objective(trial, X, y, feature_columns):\n",
    "    \"\"\"Optuna objective for hyperparameter tuning\"\"\"\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 0.9),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 0.9),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 30, 100),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.01, 1.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.01, 1.0, log=True),\n",
    "        \"objective\": \"huber\",\n",
    "        \"alpha\": 0.9,\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "        \"verbose\": -1,\n",
    "    }\n",
    "    \n",
    "    X_df = pd.DataFrame(X, columns=feature_columns)\n",
    "    \n",
    "    maes = []\n",
    "    for tr_idx, va_idx in purged_walk_forward_splits(len(X), n_splits=3):\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_df.iloc[tr_idx], y[tr_idx],\n",
    "            eval_set=[(X_df.iloc[va_idx], y[va_idx])],\n",
    "            callbacks=[lgb.early_stopping(50, verbose=False)],\n",
    "        )\n",
    "        preds = model.predict(X_df.iloc[va_idx])\n",
    "        maes.append(mean_absolute_error(y[va_idx], preds))\n",
    "    \n",
    "    return np.mean(maes)\n",
    "\n",
    "\n",
    "def optimize_hyperparameters(X, y, feature_columns, n_trials=30):\n",
    "    \"\"\"Run Optuna optimization\"\"\"\n",
    "    if len(X) < 5000:\n",
    "        print(\"Not enough data for optimization, using defaults\")\n",
    "        return None\n",
    "    \n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X, y, feature_columns),\n",
    "        n_trials=n_trials,\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"Best MAE: {study.best_value:.4f}\")\n",
    "    print(f\"Best params: {study.best_params}\")\n",
    "    \n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters on up_high_300 (largest dataset)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e129a81dee402fa3fabb9f0f6d6f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MAE: 0.6362\n",
      "Best params: {'n_estimators': 1321, 'max_depth': 5, 'learning_rate': 0.04676567448026787, 'subsample': 0.751850610914496, 'colsample_bytree': 0.898420372470383, 'min_child_samples': 30, 'reg_alpha': 0.13123125638807445, 'reg_lambda': 0.4057147503158698}\n"
     ]
    }
   ],
   "source": [
    "# Optimize for one regime to get base params (saves time)\n",
    "# Use 300s horizon data for optimization (more signal)\n",
    "print(\"Optimizing hyperparameters on up_high_300 (largest dataset)...\")\n",
    "best_params = optimize_hyperparameters(\n",
    "    X_300[\"up_high\"], \n",
    "    y_300[\"up_high\"], \n",
    "    FEATURE_COLUMNS,\n",
    "    n_trials=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Final Models with Optimized Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble_model(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    name: str,\n",
    "    feature_columns: List[str],\n",
    "    best_params: Optional[Dict] = None,\n",
    "    n_splits: int = 5\n",
    ") -> Tuple[List, Dict]:\n",
    "    \"\"\"Train ensemble with optimized params\"\"\"\n",
    "    if len(X) < 1000:\n",
    "        print(f\"Insufficient data for {name}: {len(X)} samples\")\n",
    "        return [], {}\n",
    "    \n",
    "    X_df = pd.DataFrame(X, columns=feature_columns)\n",
    "    \n",
    "    # Use best params or defaults\n",
    "    params = {\n",
    "        \"n_estimators\": best_params.get(\"n_estimators\", 1000) if best_params else 1000,\n",
    "        \"max_depth\": best_params.get(\"max_depth\", 7) if best_params else 7,\n",
    "        \"learning_rate\": best_params.get(\"learning_rate\", 0.02) if best_params else 0.02,\n",
    "        \"subsample\": best_params.get(\"subsample\", 0.7) if best_params else 0.7,\n",
    "        \"colsample_bytree\": best_params.get(\"colsample_bytree\", 0.7) if best_params else 0.7,\n",
    "        \"min_child_samples\": best_params.get(\"min_child_samples\", 50) if best_params else 50,\n",
    "        \"reg_alpha\": best_params.get(\"reg_alpha\", 0.1) if best_params else 0.1,\n",
    "        \"reg_lambda\": best_params.get(\"reg_lambda\", 0.1) if best_params else 0.1,\n",
    "        \"objective\": \"huber\",\n",
    "        \"alpha\": 0.9,\n",
    "        \"n_jobs\": -1,\n",
    "        \"verbose\": -1,\n",
    "    }\n",
    "    \n",
    "    models = []\n",
    "    metrics = {\"maes\": [], \"rmses\": [], \"top10_actual\": [], \"top25_actual\": []}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name} ({len(X):,} samples)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for fold, (tr_idx, va_idx) in enumerate(purged_walk_forward_splits(len(X_df), n_splits)):\n",
    "        model = lgb.LGBMRegressor(**params, random_state=42 + fold)\n",
    "        \n",
    "        model.fit(\n",
    "            X_df.iloc[tr_idx], y[tr_idx],\n",
    "            eval_set=[(X_df.iloc[va_idx], y[va_idx])],\n",
    "            eval_metric=\"l1\",\n",
    "            callbacks=[lgb.early_stopping(100, verbose=False)],\n",
    "        )\n",
    "        \n",
    "        preds = model.predict(X_df.iloc[va_idx])\n",
    "        actual = y[va_idx]\n",
    "        \n",
    "        mae = mean_absolute_error(actual, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(actual, preds))\n",
    "        \n",
    "        # Top percentile analysis\n",
    "        for q, key in [(90, \"top10_actual\"), (75, \"top25_actual\")]:\n",
    "            thresh = np.percentile(preds, q)\n",
    "            mask = preds >= thresh\n",
    "            if mask.sum() > 0:\n",
    "                metrics[key].append(actual[mask].mean())\n",
    "        \n",
    "        metrics[\"maes\"].append(mae)\n",
    "        metrics[\"rmses\"].append(rmse)\n",
    "        models.append(model)\n",
    "        \n",
    "        print(f\"Fold {fold}: MAE={mae:.4f}, RMSE={rmse:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{name} Summary:\")\n",
    "    print(f\"  Mean MAE: {np.mean(metrics['maes']):.4f}\")\n",
    "    print(f\"  Mean RMSE: {np.mean(metrics['rmses']):.4f}\")\n",
    "    print(f\"  Target STD: {np.std(y):.4f}\")\n",
    "    print(f\"  MAE/STD: {np.mean(metrics['maes'])/np.std(y):.4f}\")\n",
    "    if metrics['top10_actual']:\n",
    "        print(f\"  Top 10% mean actual: {np.mean(metrics['top10_actual']):.4f}\")\n",
    "        print(f\"  Top 25% mean actual: {np.mean(metrics['top25_actual']):.4f}\")\n",
    "    \n",
    "    return models, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING 60-SECOND HORIZON MODELS\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "Training UP_LOW_60 (797,579 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7686, RMSE=0.9649\n",
      "Fold 1: MAE=0.7489, RMSE=0.9172\n",
      "Fold 2: MAE=0.7337, RMSE=0.9062\n",
      "Fold 3: MAE=0.6859, RMSE=0.8470\n",
      "Fold 4: MAE=0.6535, RMSE=0.8266\n",
      "\n",
      "UP_LOW_60 Summary:\n",
      "  Mean MAE: 0.7181\n",
      "  Mean RMSE: 0.8924\n",
      "  Target STD: 0.9038\n",
      "  MAE/STD: 0.7945\n",
      "  Top 10% mean actual: 2.1142\n",
      "  Top 25% mean actual: 2.1264\n",
      "\n",
      "============================================================\n",
      "Training UP_MID_60 (651,961 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7860, RMSE=0.9809\n",
      "Fold 1: MAE=0.7422, RMSE=0.9075\n",
      "Fold 2: MAE=0.7297, RMSE=0.8878\n",
      "Fold 3: MAE=0.6751, RMSE=0.8306\n",
      "Fold 4: MAE=0.6618, RMSE=0.8366\n",
      "\n",
      "UP_MID_60 Summary:\n",
      "  Mean MAE: 0.7190\n",
      "  Mean RMSE: 0.8887\n",
      "  Target STD: 0.9006\n",
      "  MAE/STD: 0.7984\n",
      "  Top 10% mean actual: 2.2187\n",
      "  Top 25% mean actual: 2.1820\n",
      "\n",
      "============================================================\n",
      "Training UP_HIGH_60 (1,621,959 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7310, RMSE=0.9016\n",
      "Fold 1: MAE=0.7497, RMSE=0.9174\n",
      "Fold 2: MAE=0.6962, RMSE=0.8651\n",
      "Fold 3: MAE=0.6479, RMSE=0.7958\n",
      "Fold 4: MAE=0.5976, RMSE=0.7501\n",
      "\n",
      "UP_HIGH_60 Summary:\n",
      "  Mean MAE: 0.6845\n",
      "  Mean RMSE: 0.8460\n",
      "  Target STD: 0.9017\n",
      "  MAE/STD: 0.7591\n",
      "  Top 10% mean actual: 2.4809\n",
      "  Top 25% mean actual: 2.3342\n",
      "\n",
      "============================================================\n",
      "Training DOWN_LOW_60 (799,539 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7715, RMSE=0.9675\n",
      "Fold 1: MAE=0.7713, RMSE=0.9456\n",
      "Fold 2: MAE=0.7274, RMSE=0.8946\n",
      "Fold 3: MAE=0.6816, RMSE=0.8366\n",
      "Fold 4: MAE=0.6749, RMSE=0.8535\n",
      "\n",
      "DOWN_LOW_60 Summary:\n",
      "  Mean MAE: 0.7254\n",
      "  Mean RMSE: 0.8995\n",
      "  Target STD: 0.9073\n",
      "  MAE/STD: 0.7995\n",
      "  Top 10% mean actual: 2.1558\n",
      "  Top 25% mean actual: 2.1324\n",
      "\n",
      "============================================================\n",
      "Training DOWN_MID_60 (672,775 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7717, RMSE=0.9657\n",
      "Fold 1: MAE=0.7623, RMSE=0.9310\n",
      "Fold 2: MAE=0.7404, RMSE=0.9016\n",
      "Fold 3: MAE=0.6887, RMSE=0.8394\n",
      "Fold 4: MAE=0.6627, RMSE=0.8384\n",
      "\n",
      "DOWN_MID_60 Summary:\n",
      "  Mean MAE: 0.7252\n",
      "  Mean RMSE: 0.8952\n",
      "  Target STD: 0.9051\n",
      "  MAE/STD: 0.8012\n",
      "  Top 10% mean actual: 2.1402\n",
      "  Top 25% mean actual: 2.1147\n",
      "\n",
      "============================================================\n",
      "Training DOWN_HIGH_60 (1,639,455 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7205, RMSE=0.8913\n",
      "Fold 1: MAE=0.7363, RMSE=0.9048\n",
      "Fold 2: MAE=0.7207, RMSE=0.8897\n",
      "Fold 3: MAE=0.6581, RMSE=0.8111\n",
      "Fold 4: MAE=0.5941, RMSE=0.7365\n",
      "\n",
      "DOWN_HIGH_60 Summary:\n",
      "  Mean MAE: 0.6860\n",
      "  Mean RMSE: 0.8467\n",
      "  Target STD: 0.8885\n",
      "  MAE/STD: 0.7720\n",
      "  Top 10% mean actual: 2.3615\n",
      "  Top 25% mean actual: 2.2602\n",
      "\n",
      "======================================================================\n",
      "TRAINING 300-SECOND HORIZON MODELS\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "Training UP_LOW_300 (1,499,611 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7150, RMSE=0.9099\n",
      "Fold 1: MAE=0.7120, RMSE=0.9300\n",
      "Fold 2: MAE=0.7223, RMSE=0.9043\n",
      "Fold 3: MAE=0.6303, RMSE=0.8070\n",
      "Fold 4: MAE=0.5368, RMSE=0.6713\n",
      "\n",
      "UP_LOW_300 Summary:\n",
      "  Mean MAE: 0.6633\n",
      "  Mean RMSE: 0.8445\n",
      "  Target STD: 0.8742\n",
      "  MAE/STD: 0.7587\n",
      "  Top 10% mean actual: 3.4238\n",
      "  Top 25% mean actual: 3.3408\n",
      "\n",
      "============================================================\n",
      "Training UP_MID_300 (1,193,875 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7734, RMSE=0.9801\n",
      "Fold 1: MAE=0.6317, RMSE=0.8193\n",
      "Fold 2: MAE=0.7400, RMSE=0.9255\n",
      "Fold 3: MAE=0.6386, RMSE=0.8002\n",
      "Fold 4: MAE=0.5311, RMSE=0.6708\n",
      "\n",
      "UP_MID_300 Summary:\n",
      "  Mean MAE: 0.6629\n",
      "  Mean RMSE: 0.8392\n",
      "  Target STD: 0.8775\n",
      "  MAE/STD: 0.7555\n",
      "  Top 10% mean actual: 3.4271\n",
      "  Top 25% mean actual: 3.3678\n",
      "\n",
      "============================================================\n",
      "Training UP_HIGH_300 (2,577,591 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7248, RMSE=0.9342\n",
      "Fold 1: MAE=0.6916, RMSE=0.8726\n",
      "Fold 2: MAE=0.6948, RMSE=0.8758\n",
      "Fold 3: MAE=0.5853, RMSE=0.7388\n",
      "Fold 4: MAE=0.5359, RMSE=0.6659\n",
      "\n",
      "UP_HIGH_300 Summary:\n",
      "  Mean MAE: 0.6465\n",
      "  Mean RMSE: 0.8175\n",
      "  Target STD: 0.8670\n",
      "  MAE/STD: 0.7457\n",
      "  Top 10% mean actual: 3.3955\n",
      "  Top 25% mean actual: 3.2812\n",
      "\n",
      "============================================================\n",
      "Training DOWN_LOW_300 (1,487,150 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7120, RMSE=0.9120\n",
      "Fold 1: MAE=0.6910, RMSE=0.9157\n",
      "Fold 2: MAE=0.7555, RMSE=0.9392\n",
      "Fold 3: MAE=0.6186, RMSE=0.7809\n",
      "Fold 4: MAE=0.5709, RMSE=0.7158\n",
      "\n",
      "DOWN_LOW_300 Summary:\n",
      "  Mean MAE: 0.6696\n",
      "  Mean RMSE: 0.8527\n",
      "  Target STD: 0.8823\n",
      "  MAE/STD: 0.7590\n",
      "  Top 10% mean actual: 3.5445\n",
      "  Top 25% mean actual: 3.3968\n",
      "\n",
      "============================================================\n",
      "Training DOWN_MID_300 (1,202,708 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7934, RMSE=0.9994\n",
      "Fold 1: MAE=0.6350, RMSE=0.8105\n",
      "Fold 2: MAE=0.7283, RMSE=0.9099\n",
      "Fold 3: MAE=0.6260, RMSE=0.7889\n",
      "Fold 4: MAE=0.5313, RMSE=0.6670\n",
      "\n",
      "DOWN_MID_300 Summary:\n",
      "  Mean MAE: 0.6628\n",
      "  Mean RMSE: 0.8351\n",
      "  Target STD: 0.8837\n",
      "  MAE/STD: 0.7500\n",
      "  Top 10% mean actual: 3.3991\n",
      "  Top 25% mean actual: 3.3513\n",
      "\n",
      "============================================================\n",
      "Training DOWN_HIGH_300 (2,544,180 samples)\n",
      "============================================================\n",
      "Fold 0: MAE=0.7156, RMSE=0.9209\n",
      "Fold 1: MAE=0.6887, RMSE=0.8628\n",
      "Fold 2: MAE=0.6910, RMSE=0.8823\n",
      "Fold 3: MAE=0.5898, RMSE=0.7517\n",
      "Fold 4: MAE=0.5475, RMSE=0.6858\n",
      "\n",
      "DOWN_HIGH_300 Summary:\n",
      "  Mean MAE: 0.6465\n",
      "  Mean RMSE: 0.8207\n",
      "  Target STD: 0.8595\n",
      "  MAE/STD: 0.7522\n",
      "  Top 10% mean actual: 3.0679\n",
      "  Top 25% mean actual: 3.0971\n"
     ]
    }
   ],
   "source": [
    "# Train 60s horizon models\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING 60-SECOND HORIZON MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "models_60 = {}\n",
    "for key in [\"up_low\", \"up_mid\", \"up_high\", \"down_low\", \"down_mid\", \"down_high\"]:\n",
    "    models, metrics = train_ensemble_model(\n",
    "        X_60[key], y_60[key],\n",
    "        f\"{key.upper()}_60\",\n",
    "        FEATURE_COLUMNS,\n",
    "        best_params=best_params,\n",
    "    )\n",
    "    models_60[key] = models\n",
    "\n",
    "# Train 300s horizon models\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING 300-SECOND HORIZON MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "models_300 = {}\n",
    "for key in [\"up_low\", \"up_mid\", \"up_high\", \"down_low\", \"down_mid\", \"down_high\"]:\n",
    "    models, metrics = train_ensemble_model(\n",
    "        X_300[key], y_300[key],\n",
    "        f\"{key.upper()}_300\",\n",
    "        FEATURE_COLUMNS,\n",
    "        best_params=best_params,\n",
    "    )\n",
    "    models_300[key] = models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Features (300s models):\n",
      "feature\n",
      "vol_5m                   136.166667\n",
      "dist_poc                 132.833333\n",
      "dist_poc_atr             117.000000\n",
      "hour_cos                 104.200000\n",
      "hour_sin                 104.133333\n",
      "dist_lvn                 100.633333\n",
      "cum_delta_5m              96.233333\n",
      "dist_lvn_atr              89.966667\n",
      "vol_ratio                 61.400000\n",
      "trade_intensity           44.000000\n",
      "vol_1m                    39.733333\n",
      "is_weekend                36.500000\n",
      "cum_delta_1m              35.200000\n",
      "MOI_flip_rate             21.100000\n",
      "vol_rank                  19.700000\n",
      "pair_SOLUSDT              12.866667\n",
      "absorption_z              12.066667\n",
      "pair_BNBUSDT              10.566667\n",
      "pair_BTCUSDT               9.600000\n",
      "AggressionPersistence      8.500000\n",
      "Name: importance, dtype: float64\n",
      "\n",
      "Top 20 Features (60s models):\n",
      "feature\n",
      "cum_delta_5m             98.933333\n",
      "vol_5m                   98.800000\n",
      "dist_poc                 93.566667\n",
      "dist_poc_atr             85.966667\n",
      "vol_ratio                78.366667\n",
      "trade_intensity          76.733333\n",
      "hour_cos                 68.966667\n",
      "hour_sin                 67.466667\n",
      "dist_lvn                 64.833333\n",
      "cum_delta_1m             61.966667\n",
      "dist_lvn_atr             60.633333\n",
      "vol_1m                   50.966667\n",
      "vol_rank                 21.233333\n",
      "AggressionPersistence    19.433333\n",
      "MOI_flip_rate            16.766667\n",
      "pair_SOLUSDT             15.933333\n",
      "is_weekend               15.633333\n",
      "vol_5m_rank              10.433333\n",
      "trade_intensity_z        10.066667\n",
      "MOI_5s                    9.800000\n",
      "Name: importance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def get_ensemble_feature_importance(models_dict: Dict, feature_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Average feature importance across all models\"\"\"\n",
    "    all_importances = []\n",
    "    \n",
    "    for key, models in models_dict.items():\n",
    "        for model in models:\n",
    "            imp = pd.DataFrame({\n",
    "                \"feature\": feature_cols,\n",
    "                \"importance\": model.feature_importances_,\n",
    "                \"model\": key\n",
    "            })\n",
    "            all_importances.append(imp)\n",
    "    \n",
    "    if not all_importances:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df_imp = pd.concat(all_importances)\n",
    "    return df_imp.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False)\n",
    "\n",
    "# Feature importance for 300s models (main horizon)\n",
    "print(\"Top 20 Features (300s models):\")\n",
    "fi_300 = get_ensemble_feature_importance(models_300, FEATURE_COLUMNS)\n",
    "print(fi_300.head(20))\n",
    "\n",
    "print(\"\\nTop 20 Features (60s models):\")\n",
    "fi_60 = get_ensemble_feature_importance(models_60, FEATURE_COLUMNS)\n",
    "print(fi_60.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 60s models...\n",
      "  Saved models_v3/models_up_low_60.pkl\n",
      "  Saved models_v3/models_up_mid_60.pkl\n",
      "  Saved models_v3/models_up_high_60.pkl\n",
      "  Saved models_v3/models_down_low_60.pkl\n",
      "  Saved models_v3/models_down_mid_60.pkl\n",
      "  Saved models_v3/models_down_high_60.pkl\n",
      "\n",
      "Saving 300s models...\n",
      "  Saved models_v3/models_up_low_300.pkl\n",
      "  Saved models_v3/models_up_mid_300.pkl\n",
      "  Saved models_v3/models_up_high_300.pkl\n",
      "  Saved models_v3/models_down_low_300.pkl\n",
      "  Saved models_v3/models_down_mid_300.pkl\n",
      "  Saved models_v3/models_down_high_300.pkl\n",
      "\n",
      "Saved best_params.json\n",
      "Saved feature_columns_v3.json\n",
      "\n",
      "============================================================\n",
      "Done! Models saved to models_v3/\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models_v3\", exist_ok=True)\n",
    "\n",
    "# Save 60s models\n",
    "print(\"Saving 60s models...\")\n",
    "for key, models in models_60.items():\n",
    "    if models:\n",
    "        path = f\"models_v3/models_{key}_60.pkl\"\n",
    "        joblib.dump(models, path)\n",
    "        print(f\"  Saved {path}\")\n",
    "\n",
    "# Save 300s models\n",
    "print(\"\\nSaving 300s models...\")\n",
    "for key, models in models_300.items():\n",
    "    if models:\n",
    "        path = f\"models_v3/models_{key}_300.pkl\"\n",
    "        joblib.dump(models, path)\n",
    "        print(f\"  Saved {path}\")\n",
    "\n",
    "# Save best hyperparameters\n",
    "if best_params:\n",
    "    with open(\"models_v3/best_params.json\", \"w\") as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "    print(\"\\nSaved best_params.json\")\n",
    "\n",
    "# Copy feature columns\n",
    "import shutil\n",
    "shutil.copy(\"feature_columns_v3.json\", \"models_v3/feature_columns_v3.json\")\n",
    "print(\"Saved feature_columns_v3.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Done! Models saved to models_v3/\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**V3 Enhanced with Improved Labeling:**\n",
    "\n",
    "1. **Fee-Adjusted Max Favorable Move Labels** (from V3 Improved) - cleaner signal than triple-barrier\n",
    "2. **Dual Horizons**: 60s and 300s models for different timeframes\n",
    "3. **Cross-Sectional Features**: 5 rank-based features comparing symbols\n",
    "4. **Optuna Optimization**: 25 trials for hyperparameter tuning\n",
    "5. **21 Days Data**: More training data for robustness\n",
    "6. **41 Features**: Extended feature set with cross-sectional ranks\n",
    "\n",
    "**Expected Improvements over previous V3 Enhanced:**\n",
    "- MAE/STD ratio: ~0.70-0.75 (was 0.83-0.89)\n",
    "- Top 10% actual: ~2.5-4.0 (was 1.8-2.2)\n",
    "- Cleaner label distribution with profitable samples only\n",
    "\n",
    "**To use in production:**\n",
    "1. Copy `models_v3/*.pkl` to production server\n",
    "2. Update predictor to use `feature_columns_v3.json`\n",
    "3. Compute cross-sectional features in real-time (or use fallback 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
