{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydra V3 Enhanced - ML Model Training\n",
    "\n",
    "## Enhancements over V2:\n",
    "1. **Cross-Sectional Features**: Rank-based features across symbols\n",
    "2. **Triple-Barrier Labels**: Realistic TP/SL/Time-based targets\n",
    "3. **Optuna Hyperparameter Optimization**: Per-regime tuning\n",
    "4. **21 Days of Data**: More robust training\n",
    "5. **Memory Optimized**: Chunked processing, float32 throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy requests pyarrow lightgbm scikit-learn tqdm scipy optuna -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import time\n",
    "import gc\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PAIRS = [\n",
    "    \"BTCUSDT\", \"ETHUSDT\", \"SOLUSDT\", \"BNBUSDT\",\n",
    "    \"XRPUSDT\", \"DOGEUSDT\", \"LTCUSDT\", \"ADAUSDT\",\n",
    "]\n",
    "\n",
    "DAYS = 21  # Balanced: more data without memory issues\n",
    "FEE_PCT = 0.0004  # 0.04% per side\n",
    "ROUND_TRIP_FEE = 2 * FEE_PCT  # 0.08%\n",
    "\n",
    "# Triple Barrier Parameters\n",
    "TP_MULT = 2.0  # Take profit at 2x ATR\n",
    "SL_MULT = 1.0  # Stop loss at 1x ATR\n",
    "MAX_HOLDING_BARS = 1200  # 5 minutes max hold (300s / 0.25s per bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Fetching (Memory Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_aggtrades_day(symbol: str, date: datetime) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Fetch aggregated trades for a single day\"\"\"\n",
    "    date_str = date.strftime(\"%Y-%m-%d\")\n",
    "    url = (\n",
    "        f\"https://data.binance.vision/data/futures/um/daily/aggTrades/\"\n",
    "        f\"{symbol}/{symbol}-aggTrades-{date_str}.zip\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        \n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        csv_name = z.namelist()[0]\n",
    "        df = pd.read_csv(z.open(csv_name))\n",
    "        df[\"symbol\"] = symbol\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {symbol} {date_str}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_symbol_data(symbol: str, days: int) -> pd.DataFrame:\n",
    "    \"\"\"Fetch all data for a single symbol\"\"\"\n",
    "    all_dfs = []\n",
    "    end_date = datetime.now(timezone.utc).date() - timedelta(days=1)\n",
    "    start_date = end_date - timedelta(days=days)\n",
    "    \n",
    "    for i in tqdm(range(days), desc=symbol):\n",
    "        day = start_date + timedelta(days=i)\n",
    "        df_day = fetch_aggtrades_day(symbol, day)\n",
    "        if df_day is not None:\n",
    "            all_dfs.append(df_day)\n",
    "    \n",
    "    if not all_dfs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    # Clean and convert to efficient types\n",
    "    df = df.rename(columns={\n",
    "        \"transact_time\": \"timestamp\",\n",
    "        \"is_buyer_maker\": \"is_sell\"\n",
    "    })\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "    df[\"price\"] = df[\"price\"].astype(\"float32\")\n",
    "    df[\"quantity\"] = df[\"quantity\"].astype(\"float32\")\n",
    "    df[\"is_sell\"] = df[\"is_sell\"].astype(\"int8\")\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data per symbol to manage memory\n",
    "symbol_data = {}\n",
    "for symbol in PAIRS:\n",
    "    print(f\"\\nFetching {symbol}\")\n",
    "    symbol_data[symbol] = fetch_symbol_data(symbol, DAYS)\n",
    "    print(f\"  {len(symbol_data[symbol]):,} trades\")\n",
    "\n",
    "total_trades = sum(len(df) for df in symbol_data.values())\n",
    "print(f\"\\nTotal trades: {total_trades:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhanced Feature Engineering with Cross-Sectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_base_features(df_sym: pd.DataFrame, symbol: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute base features for a single symbol.\n",
    "    Cross-sectional features added later across all symbols.\n",
    "    \"\"\"\n",
    "    df_sym = df_sym.copy()\n",
    "    df_sym[\"signed_qty\"] = np.where(df_sym[\"is_sell\"], -df_sym[\"quantity\"], df_sym[\"quantity\"])\n",
    "    \n",
    "    # Resample to 250ms bars\n",
    "    bars = (\n",
    "        df_sym\n",
    "        .set_index(\"timestamp\")\n",
    "        .resample(\"250ms\")\n",
    "        .agg(\n",
    "            price=(\"price\", \"last\"),\n",
    "            qty=(\"quantity\", \"sum\"),\n",
    "            signed_qty=(\"signed_qty\", \"sum\"),\n",
    "            trade_count=(\"quantity\", \"count\"),\n",
    "        )\n",
    "        .dropna(subset=[\"price\"])\n",
    "    )\n",
    "    bars[\"price\"] = bars[\"price\"].ffill()\n",
    "    bars = bars.reset_index()\n",
    "    bars[\"symbol\"] = symbol\n",
    "    \n",
    "    # ============ ORDER FLOW FEATURES ============\n",
    "    bars[\"MOI_250ms\"] = bars[\"signed_qty\"].rolling(1).sum()\n",
    "    bars[\"MOI_1s\"] = bars[\"signed_qty\"].rolling(4).sum()\n",
    "    bars[\"MOI_5s\"] = bars[\"signed_qty\"].rolling(20).sum()\n",
    "    bars[\"MOI_std\"] = bars[\"MOI_1s\"].rolling(100).std()\n",
    "    bars[\"MOI_z\"] = bars[\"MOI_1s\"].abs() / (bars[\"MOI_std\"] + 1e-6)\n",
    "    bars[\"delta_velocity\"] = bars[\"MOI_1s\"].diff()\n",
    "    bars[\"delta_velocity_5s\"] = bars[\"MOI_1s\"].diff(20)\n",
    "    \n",
    "    # Aggression persistence\n",
    "    abs_moi = bars[\"MOI_1s\"].abs()\n",
    "    mean_moi = abs_moi.rolling(100).mean()\n",
    "    std_moi = abs_moi.rolling(100).std()\n",
    "    bars[\"AggressionPersistence\"] = mean_moi / (std_moi + 1e-6)\n",
    "    \n",
    "    # MOI flip rate\n",
    "    moi_sign = np.sign(bars[\"MOI_1s\"])\n",
    "    sign_change = (moi_sign != moi_sign.shift(1)).astype(int)\n",
    "    bars[\"MOI_flip_rate\"] = sign_change.rolling(240).sum()\n",
    "    \n",
    "    # Order flow momentum\n",
    "    bars[\"MOI_roc_1s\"] = bars[\"MOI_1s\"].pct_change(4).clip(-10, 10)\n",
    "    bars[\"MOI_roc_5s\"] = bars[\"MOI_1s\"].pct_change(20).clip(-10, 10)\n",
    "    bars[\"MOI_acceleration\"] = bars[\"delta_velocity\"].diff()\n",
    "    \n",
    "    # ============ ABSORPTION FEATURES ============\n",
    "    price_change = bars[\"price\"].diff().abs().clip(lower=1e-6)\n",
    "    bars[\"absorption_raw\"] = bars[\"qty\"] / price_change\n",
    "    bars[\"absorption_z\"] = (\n",
    "        (bars[\"absorption_raw\"] - bars[\"absorption_raw\"].rolling(500).mean()) /\n",
    "        (bars[\"absorption_raw\"].rolling(500).std() + 1e-6)\n",
    "    )\n",
    "    bars[\"price_impact\"] = price_change / (bars[\"qty\"] + 1e-6)\n",
    "    bars[\"price_impact_z\"] = (\n",
    "        (bars[\"price_impact\"] - bars[\"price_impact\"].rolling(500).mean()) /\n",
    "        (bars[\"price_impact\"].rolling(500).std() + 1e-6)\n",
    "    )\n",
    "    \n",
    "    # ============ VOLATILITY FEATURES ============\n",
    "    bars[\"ret\"] = bars[\"price\"].pct_change()\n",
    "    bars[\"vol_1m\"] = bars[\"ret\"].rolling(240).std()\n",
    "    bars[\"vol_5m\"] = bars[\"ret\"].rolling(1200).std()\n",
    "    bars[\"vol_ratio\"] = bars[\"vol_1m\"] / (bars[\"vol_5m\"] + 1e-8)\n",
    "    bars[\"vol_rank\"] = bars[\"vol_5m\"].rolling(2000).rank(pct=True)\n",
    "    \n",
    "    # ATR for triple barrier\n",
    "    bars[\"atr_5m\"] = bars[\"ret\"].abs().rolling(1200).mean() * bars[\"price\"]\n",
    "    \n",
    "    # Vol regime\n",
    "    bars[\"vol_regime\"] = pd.cut(\n",
    "        bars[\"vol_rank\"],\n",
    "        bins=[-np.inf, 0.3, 0.7, np.inf],\n",
    "        labels=[\"LOW\", \"MID\", \"HIGH\"]\n",
    "    )\n",
    "    \n",
    "    # ============ STRUCTURE FEATURES ============\n",
    "    BIN_SIZE = 10\n",
    "    LVN_BLOCK = 1200\n",
    "    \n",
    "    bars[\"price_bin\"] = (bars[\"price\"] / BIN_SIZE).round() * BIN_SIZE\n",
    "    lvn_price = np.full(len(bars), np.nan)\n",
    "    poc_price = np.full(len(bars), np.nan)\n",
    "    \n",
    "    for i in range(0, len(bars), LVN_BLOCK):\n",
    "        window = bars.iloc[i:i+LVN_BLOCK]\n",
    "        if window[\"qty\"].sum() == 0:\n",
    "            continue\n",
    "        vp = window.groupby(\"price_bin\")[\"qty\"].sum()\n",
    "        lvn_price[i:i+LVN_BLOCK] = vp.idxmin()\n",
    "        poc_price[i:i+LVN_BLOCK] = vp.idxmax()\n",
    "    \n",
    "    bars[\"LVN_price\"] = lvn_price\n",
    "    bars[\"POC_price\"] = poc_price\n",
    "    bars[\"dist_lvn\"] = (bars[\"price\"] - bars[\"LVN_price\"]).abs()\n",
    "    bars[\"dist_poc\"] = (bars[\"price\"] - bars[\"POC_price\"]).abs()\n",
    "    bars[\"dist_lvn_atr\"] = bars[\"dist_lvn\"] / (bars[\"atr_5m\"] + 1e-6)\n",
    "    bars[\"dist_poc_atr\"] = bars[\"dist_poc\"] / (bars[\"atr_5m\"] + 1e-6)\n",
    "    \n",
    "    # ============ TIME FEATURES ============\n",
    "    bars[\"hour\"] = bars[\"timestamp\"].dt.hour\n",
    "    bars[\"hour_sin\"] = np.sin(2 * np.pi * bars[\"hour\"] / 24)\n",
    "    bars[\"hour_cos\"] = np.cos(2 * np.pi * bars[\"hour\"] / 24)\n",
    "    bars[\"is_weekend\"] = (bars[\"timestamp\"].dt.dayofweek >= 5).astype(int)\n",
    "    \n",
    "    # ============ TRADE INTENSITY ============\n",
    "    bars[\"trade_intensity\"] = bars[\"trade_count\"].rolling(100).mean()\n",
    "    bars[\"trade_intensity_z\"] = (\n",
    "        (bars[\"trade_count\"] - bars[\"trade_count\"].rolling(500).mean()) /\n",
    "        (bars[\"trade_count\"].rolling(500).std() + 1e-6)\n",
    "    )\n",
    "    \n",
    "    # ============ CUMULATIVE FEATURES ============\n",
    "    bars[\"cum_delta_1m\"] = bars[\"signed_qty\"].rolling(240).sum()\n",
    "    bars[\"cum_delta_5m\"] = bars[\"signed_qty\"].rolling(1200).sum()\n",
    "    \n",
    "    # Convert to float32\n",
    "    float_cols = bars.select_dtypes(include=[np.float64]).columns\n",
    "    bars[float_cols] = bars[float_cols].astype(np.float32)\n",
    "    \n",
    "    return bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all symbols\n",
    "all_bars = {}\n",
    "\n",
    "for symbol in PAIRS:\n",
    "    print(f\"Processing {symbol}\")\n",
    "    if len(symbol_data[symbol]) > 0:\n",
    "        all_bars[symbol] = compute_base_features(symbol_data[symbol], symbol)\n",
    "        print(f\"  {len(all_bars[symbol]):,} bars\")\n",
    "    \n",
    "    # Free memory\n",
    "    del symbol_data[symbol]\n",
    "    gc.collect()\n",
    "\n",
    "del symbol_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cross_sectional_features(all_bars: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Add cross-sectional features: rank features across all symbols at each timestamp.\n",
    "    \n",
    "    This captures relative strength - which symbol is leading/lagging.\n",
    "    \"\"\"\n",
    "    print(\"Adding cross-sectional features...\")\n",
    "    \n",
    "    # Features to rank across symbols\n",
    "    rank_features = [\"MOI_1s\", \"MOI_5s\", \"vol_5m\", \"absorption_z\", \"cum_delta_5m\"]\n",
    "    \n",
    "    # Get common timestamps (rounded to 250ms)\n",
    "    for symbol, bars in all_bars.items():\n",
    "        bars[\"ts_key\"] = bars[\"timestamp\"].dt.floor(\"250ms\")\n",
    "    \n",
    "    # For each feature, compute rank across symbols\n",
    "    for feature in tqdm(rank_features, desc=\"Cross-sectional\"):\n",
    "        # Build cross-sectional dataframe\n",
    "        cross_df = pd.DataFrame()\n",
    "        for symbol, bars in all_bars.items():\n",
    "            temp = bars[[\"ts_key\", feature]].copy()\n",
    "            temp = temp.rename(columns={feature: symbol})\n",
    "            if cross_df.empty:\n",
    "                cross_df = temp\n",
    "            else:\n",
    "                cross_df = cross_df.merge(temp, on=\"ts_key\", how=\"outer\")\n",
    "        \n",
    "        # Compute rank (0-1) across symbols for each timestamp\n",
    "        symbol_cols = [s for s in PAIRS if s in cross_df.columns]\n",
    "        cross_df[\"rank_data\"] = cross_df[symbol_cols].rank(axis=1, pct=True)\n",
    "        \n",
    "        # Add rank back to each symbol's bars\n",
    "        for symbol in symbol_cols:\n",
    "            rank_col = f\"{feature}_rank\"\n",
    "            # Get rank for this symbol\n",
    "            symbol_ranks = cross_df[[\"ts_key\", symbol]].copy()\n",
    "            symbol_ranks[rank_col] = cross_df[symbol_cols].rank(axis=1, pct=True)[symbol]\n",
    "            symbol_ranks = symbol_ranks[[\"ts_key\", rank_col]]\n",
    "            \n",
    "            # Merge back\n",
    "            all_bars[symbol] = all_bars[symbol].merge(\n",
    "                symbol_ranks, on=\"ts_key\", how=\"left\"\n",
    "            )\n",
    "            all_bars[symbol][rank_col] = all_bars[symbol][rank_col].fillna(0.5).astype(np.float32)\n",
    "        \n",
    "        del cross_df\n",
    "        gc.collect()\n",
    "    \n",
    "    # Remove ts_key\n",
    "    for symbol in all_bars:\n",
    "        all_bars[symbol] = all_bars[symbol].drop(columns=[\"ts_key\"])\n",
    "    \n",
    "    return all_bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bars = add_cross_sectional_features(all_bars)\n",
    "print(f\"\\nFeatures per symbol: {len(all_bars[PAIRS[0]].columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection and Decision Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended feature columns with cross-sectional\n",
    "FEATURE_COLS = [\n",
    "    # Order flow (7)\n",
    "    \"MOI_250ms\", \"MOI_1s\", \"MOI_5s\", \"MOI_z\",\n",
    "    \"delta_velocity\", \"delta_velocity_5s\", \"AggressionPersistence\",\n",
    "    \n",
    "    # Order flow momentum (3)\n",
    "    \"MOI_roc_1s\", \"MOI_roc_5s\", \"MOI_acceleration\",\n",
    "    \n",
    "    # Absorption (3)\n",
    "    \"absorption_z\", \"price_impact_z\", \"MOI_flip_rate\",\n",
    "    \n",
    "    # Volatility (4)\n",
    "    \"vol_1m\", \"vol_5m\", \"vol_ratio\", \"vol_rank\",\n",
    "    \n",
    "    # Structure (4)\n",
    "    \"dist_lvn\", \"dist_poc\", \"dist_lvn_atr\", \"dist_poc_atr\",\n",
    "    \n",
    "    # Time (3)\n",
    "    \"hour_sin\", \"hour_cos\", \"is_weekend\",\n",
    "    \n",
    "    # Trade intensity (2)\n",
    "    \"trade_intensity\", \"trade_intensity_z\",\n",
    "    \n",
    "    # Cumulative (2)\n",
    "    \"cum_delta_1m\", \"cum_delta_5m\",\n",
    "    \n",
    "    # Cross-sectional ranks (5) - NEW\n",
    "    \"MOI_1s_rank\", \"MOI_5s_rank\", \"vol_5m_rank\", \n",
    "    \"absorption_z_rank\", \"cum_delta_5m_rank\",\n",
    "]\n",
    "\n",
    "print(f\"Total features: {len(FEATURE_COLS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision points with stricter filtering\n",
    "all_decisions = []\n",
    "\n",
    "for symbol in PAIRS:\n",
    "    print(f\"Creating decision points for {symbol}\")\n",
    "    \n",
    "    bars_sym = all_bars[symbol].copy()\n",
    "    bars_sym = bars_sym.dropna(subset=FEATURE_COLS)\n",
    "    \n",
    "    # Adaptive thresholds\n",
    "    bars_sym[\"MOI_thresh\"] = bars_sym[\"MOI_1s\"].abs().rolling(2000).quantile(0.85)\n",
    "    bars_sym[\"LVN_thresh\"] = bars_sym[\"dist_lvn_atr\"].rolling(2000).quantile(0.15)\n",
    "    bars_sym[\"absorption_thresh\"] = bars_sym[\"absorption_z\"].abs().rolling(2000).quantile(0.85)\n",
    "    \n",
    "    # Decision mask: require stronger conditions\n",
    "    decision_mask = (\n",
    "        (bars_sym[\"dist_lvn_atr\"] < bars_sym[\"LVN_thresh\"]) |  # Near LVN\n",
    "        (bars_sym[\"absorption_z\"].abs() > bars_sym[\"absorption_thresh\"]) |  # Absorption\n",
    "        (bars_sym[\"MOI_1s\"].abs() > bars_sym[\"MOI_thresh\"]) |  # Strong flow\n",
    "        (bars_sym[\"vol_ratio\"] > 1.8)  # Vol expansion\n",
    "    )\n",
    "    \n",
    "    df_decision_sym = bars_sym.loc[decision_mask].copy()\n",
    "    df_decision_sym[\"bar_idx\"] = df_decision_sym.index\n",
    "    all_decisions.append(df_decision_sym)\n",
    "    \n",
    "    print(f\"  {len(df_decision_sym):,} decision points ({100*len(df_decision_sym)/len(bars_sym):.1f}%)\")\n",
    "\n",
    "df_decision = pd.concat(all_decisions, ignore_index=True)\n",
    "print(f\"\\nTotal decision points: {len(df_decision):,}\")\n",
    "\n",
    "del all_decisions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to float32\n",
    "for col in FEATURE_COLS:\n",
    "    if col in df_decision.columns:\n",
    "        df_decision[col] = df_decision[col].astype(np.float32)\n",
    "\n",
    "# One-hot encode symbols\n",
    "pair_ohe = pd.get_dummies(df_decision[\"symbol\"], prefix=\"pair\", dtype=\"int8\")\n",
    "\n",
    "# Final feature columns\n",
    "FEATURE_COLUMNS = FEATURE_COLS + pair_ohe.columns.tolist()\n",
    "print(f\"Final feature count: {len(FEATURE_COLUMNS)}\")\n",
    "\n",
    "# Create X matrix\n",
    "X = np.hstack([\n",
    "    df_decision[FEATURE_COLS].values,\n",
    "    pair_ohe.values.astype(np.float32)\n",
    "])\n",
    "print(f\"X shape: {X.shape}, dtype: {X.dtype}\")\n",
    "print(f\"Memory: {X.nbytes / 1e9:.2f} GB\")\n",
    "\n",
    "del pair_ohe\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature columns\n",
    "with open(\"feature_columns_v3.json\", \"w\") as f:\n",
    "    json.dump(FEATURE_COLUMNS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Triple-Barrier Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_barrier_label(\n",
    "    bars: pd.DataFrame,\n",
    "    entry_idx: int,\n",
    "    direction: str,  # \"up\" or \"down\"\n",
    "    tp_mult: float = 2.0,\n",
    "    sl_mult: float = 1.0,\n",
    "    max_bars: int = 1200,\n",
    "    fee_pct: float = 0.0008,  # Round trip\n",
    ") -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Triple-barrier labeling:\n",
    "    - TP barrier: tp_mult * ATR\n",
    "    - SL barrier: sl_mult * ATR  \n",
    "    - Time barrier: max_bars\n",
    "    \n",
    "    Returns:\n",
    "    - score: Risk-adjusted return (positive = profitable)\n",
    "    - exit_type: \"TP\", \"SL\", or \"TIME\"\n",
    "    \"\"\"\n",
    "    entry_price = bars.loc[entry_idx, \"price\"]\n",
    "    atr = bars.loc[entry_idx, \"atr_5m\"]\n",
    "    \n",
    "    if pd.isna(atr) or atr <= 0:\n",
    "        return 0.0, \"SKIP\"\n",
    "    \n",
    "    # Define barriers\n",
    "    if direction == \"up\":\n",
    "        tp_price = entry_price * (1 + tp_mult * atr / entry_price)\n",
    "        sl_price = entry_price * (1 - sl_mult * atr / entry_price)\n",
    "    else:  # down\n",
    "        tp_price = entry_price * (1 - tp_mult * atr / entry_price)\n",
    "        sl_price = entry_price * (1 + sl_mult * atr / entry_price)\n",
    "    \n",
    "    # Look forward\n",
    "    end_idx = min(entry_idx + max_bars, len(bars) - 1)\n",
    "    \n",
    "    for i in range(entry_idx + 1, end_idx + 1):\n",
    "        price = bars.loc[i, \"price\"]\n",
    "        \n",
    "        if direction == \"up\":\n",
    "            if price >= tp_price:\n",
    "                pnl = (tp_price - entry_price) / entry_price - fee_pct\n",
    "                return pnl / (sl_mult * atr / entry_price), \"TP\"\n",
    "            if price <= sl_price:\n",
    "                pnl = (sl_price - entry_price) / entry_price - fee_pct\n",
    "                return pnl / (sl_mult * atr / entry_price), \"SL\"\n",
    "        else:\n",
    "            if price <= tp_price:\n",
    "                pnl = (entry_price - tp_price) / entry_price - fee_pct\n",
    "                return pnl / (sl_mult * atr / entry_price), \"TP\"\n",
    "            if price >= sl_price:\n",
    "                pnl = (entry_price - sl_price) / entry_price - fee_pct\n",
    "                return pnl / (sl_mult * atr / entry_price), \"SL\"\n",
    "    \n",
    "    # Time exit\n",
    "    exit_price = bars.loc[end_idx, \"price\"]\n",
    "    if direction == \"up\":\n",
    "        pnl = (exit_price - entry_price) / entry_price - fee_pct\n",
    "    else:\n",
    "        pnl = (entry_price - exit_price) / entry_price - fee_pct\n",
    "    \n",
    "    return pnl / (sl_mult * atr / entry_price), \"TIME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triple_barrier_labels(\n",
    "    all_bars: Dict[str, pd.DataFrame],\n",
    "    df_decision: pd.DataFrame,\n",
    "    X: np.ndarray,\n",
    ") -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Create labels using triple-barrier method.\n",
    "    Separates by direction and volatility regime.\n",
    "    \"\"\"\n",
    "    X_dict = {\n",
    "        \"up_low\": [], \"up_mid\": [], \"up_high\": [],\n",
    "        \"down_low\": [], \"down_mid\": [], \"down_high\": []\n",
    "    }\n",
    "    y_dict = {\n",
    "        \"up_low\": [], \"up_mid\": [], \"up_high\": [],\n",
    "        \"down_low\": [], \"down_mid\": [], \"down_high\": []\n",
    "    }\n",
    "    exit_stats = {\"TP\": 0, \"SL\": 0, \"TIME\": 0, \"SKIP\": 0}\n",
    "    \n",
    "    for symbol in PAIRS:\n",
    "        print(f\"Labeling {symbol}\")\n",
    "        \n",
    "        bars_sym = all_bars[symbol].reset_index(drop=True)\n",
    "        dec_sym = df_decision[df_decision[\"symbol\"] == symbol]\n",
    "        \n",
    "        for i, row in tqdm(dec_sym.iterrows(), total=len(dec_sym), desc=symbol):\n",
    "            idx = int(row[\"bar_idx\"])\n",
    "            regime = row[\"vol_regime\"]\n",
    "            \n",
    "            if pd.isna(regime):\n",
    "                continue\n",
    "            \n",
    "            # Try both directions, pick better one\n",
    "            score_up, exit_up = triple_barrier_label(\n",
    "                bars_sym, idx, \"up\", TP_MULT, SL_MULT, MAX_HOLDING_BARS, ROUND_TRIP_FEE\n",
    "            )\n",
    "            score_down, exit_down = triple_barrier_label(\n",
    "                bars_sym, idx, \"down\", TP_MULT, SL_MULT, MAX_HOLDING_BARS, ROUND_TRIP_FEE\n",
    "            )\n",
    "            \n",
    "            if exit_up == \"SKIP\" and exit_down == \"SKIP\":\n",
    "                exit_stats[\"SKIP\"] += 1\n",
    "                continue\n",
    "            \n",
    "            # Pick direction with better score\n",
    "            features = X[row.name]\n",
    "            \n",
    "            if score_up > score_down and score_up > -0.5:  # Allow slightly negative\n",
    "                key = f\"up_{regime.lower()}\"\n",
    "                X_dict[key].append(features)\n",
    "                y_dict[key].append(max(0, score_up))  # Clip to 0\n",
    "                exit_stats[exit_up] += 1\n",
    "            elif score_down > -0.5:\n",
    "                key = f\"down_{regime.lower()}\"\n",
    "                X_dict[key].append(features)\n",
    "                y_dict[key].append(max(0, score_down))\n",
    "                exit_stats[exit_down] += 1\n",
    "    \n",
    "    # Convert to numpy\n",
    "    for key in X_dict:\n",
    "        X_dict[key] = np.array(X_dict[key], dtype=np.float32)\n",
    "        y_arr = np.array(y_dict[key], dtype=np.float32)\n",
    "        # Log transform for better distribution\n",
    "        y_dict[key] = np.log1p(y_arr)\n",
    "        print(f\"{key}: {len(X_dict[key]):,} samples\")\n",
    "    \n",
    "    print(f\"\\nExit stats: {exit_stats}\")\n",
    "    return X_dict, y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = create_triple_barrier_labels(all_bars, df_decision, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optuna Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purged_walk_forward_splits(n: int, n_splits: int = 5, purge_pct: float = 0.01):\n",
    "    \"\"\"Walk-forward splits with purging\"\"\"\n",
    "    fold_size = n // (n_splits + 1)\n",
    "    purge_size = int(fold_size * purge_pct)\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        tr_end = fold_size * (i + 1) - purge_size\n",
    "        va_start = fold_size * (i + 1) + purge_size\n",
    "        va_end = fold_size * (i + 2)\n",
    "        \n",
    "        yield np.arange(0, tr_end), np.arange(va_start, va_end)\n",
    "\n",
    "\n",
    "def objective(trial, X, y, feature_columns):\n",
    "    \"\"\"Optuna objective for hyperparameter tuning\"\"\"\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 0.9),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 0.9),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 30, 100),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.01, 1.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.01, 1.0, log=True),\n",
    "        \"objective\": \"huber\",\n",
    "        \"alpha\": 0.9,\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "        \"verbose\": -1,\n",
    "    }\n",
    "    \n",
    "    X_df = pd.DataFrame(X, columns=feature_columns)\n",
    "    \n",
    "    maes = []\n",
    "    for tr_idx, va_idx in purged_walk_forward_splits(len(X), n_splits=3):\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_df.iloc[tr_idx], y[tr_idx],\n",
    "            eval_set=[(X_df.iloc[va_idx], y[va_idx])],\n",
    "            callbacks=[lgb.early_stopping(50, verbose=False)],\n",
    "        )\n",
    "        preds = model.predict(X_df.iloc[va_idx])\n",
    "        maes.append(mean_absolute_error(y[va_idx], preds))\n",
    "    \n",
    "    return np.mean(maes)\n",
    "\n",
    "\n",
    "def optimize_hyperparameters(X, y, feature_columns, n_trials=30):\n",
    "    \"\"\"Run Optuna optimization\"\"\"\n",
    "    if len(X) < 5000:\n",
    "        print(\"Not enough data for optimization, using defaults\")\n",
    "        return None\n",
    "    \n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X, y, feature_columns),\n",
    "        n_trials=n_trials,\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"Best MAE: {study.best_value:.4f}\")\n",
    "    print(f\"Best params: {study.best_params}\")\n",
    "    \n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize for one regime to get base params (saves time)\n",
    "print(\"Optimizing hyperparameters on up_high (largest dataset)...\")\n",
    "best_params = optimize_hyperparameters(\n",
    "    X_data[\"up_high\"], \n",
    "    y_data[\"up_high\"], \n",
    "    FEATURE_COLUMNS,\n",
    "    n_trials=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Final Models with Optimized Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble_model(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    name: str,\n",
    "    feature_columns: List[str],\n",
    "    best_params: Optional[Dict] = None,\n",
    "    n_splits: int = 5\n",
    ") -> Tuple[List, Dict]:\n",
    "    \"\"\"Train ensemble with optimized params\"\"\"\n",
    "    if len(X) < 1000:\n",
    "        print(f\"Insufficient data for {name}: {len(X)} samples\")\n",
    "        return [], {}\n",
    "    \n",
    "    X_df = pd.DataFrame(X, columns=feature_columns)\n",
    "    \n",
    "    # Use best params or defaults\n",
    "    params = {\n",
    "        \"n_estimators\": best_params.get(\"n_estimators\", 1000) if best_params else 1000,\n",
    "        \"max_depth\": best_params.get(\"max_depth\", 7) if best_params else 7,\n",
    "        \"learning_rate\": best_params.get(\"learning_rate\", 0.02) if best_params else 0.02,\n",
    "        \"subsample\": best_params.get(\"subsample\", 0.7) if best_params else 0.7,\n",
    "        \"colsample_bytree\": best_params.get(\"colsample_bytree\", 0.7) if best_params else 0.7,\n",
    "        \"min_child_samples\": best_params.get(\"min_child_samples\", 50) if best_params else 50,\n",
    "        \"reg_alpha\": best_params.get(\"reg_alpha\", 0.1) if best_params else 0.1,\n",
    "        \"reg_lambda\": best_params.get(\"reg_lambda\", 0.1) if best_params else 0.1,\n",
    "        \"objective\": \"huber\",\n",
    "        \"alpha\": 0.9,\n",
    "        \"n_jobs\": -1,\n",
    "        \"verbose\": -1,\n",
    "    }\n",
    "    \n",
    "    models = []\n",
    "    metrics = {\"maes\": [], \"rmses\": [], \"top10_actual\": [], \"top25_actual\": []}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name} ({len(X):,} samples)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for fold, (tr_idx, va_idx) in enumerate(purged_walk_forward_splits(len(X_df), n_splits)):\n",
    "        model = lgb.LGBMRegressor(**params, random_state=42 + fold)\n",
    "        \n",
    "        model.fit(\n",
    "            X_df.iloc[tr_idx], y[tr_idx],\n",
    "            eval_set=[(X_df.iloc[va_idx], y[va_idx])],\n",
    "            eval_metric=\"l1\",\n",
    "            callbacks=[lgb.early_stopping(100, verbose=False)],\n",
    "        )\n",
    "        \n",
    "        preds = model.predict(X_df.iloc[va_idx])\n",
    "        actual = y[va_idx]\n",
    "        \n",
    "        mae = mean_absolute_error(actual, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(actual, preds))\n",
    "        \n",
    "        # Top percentile analysis\n",
    "        for q, key in [(90, \"top10_actual\"), (75, \"top25_actual\")]:\n",
    "            thresh = np.percentile(preds, q)\n",
    "            mask = preds >= thresh\n",
    "            if mask.sum() > 0:\n",
    "                metrics[key].append(actual[mask].mean())\n",
    "        \n",
    "        metrics[\"maes\"].append(mae)\n",
    "        metrics[\"rmses\"].append(rmse)\n",
    "        models.append(model)\n",
    "        \n",
    "        print(f\"Fold {fold}: MAE={mae:.4f}, RMSE={rmse:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{name} Summary:\")\n",
    "    print(f\"  Mean MAE: {np.mean(metrics['maes']):.4f}\")\n",
    "    print(f\"  Mean RMSE: {np.mean(metrics['rmses']):.4f}\")\n",
    "    print(f\"  Target STD: {np.std(y):.4f}\")\n",
    "    print(f\"  MAE/STD: {np.mean(metrics['maes'])/np.std(y):.4f}\")\n",
    "    if metrics['top10_actual']:\n",
    "        print(f\"  Top 10% mean actual: {np.mean(metrics['top10_actual']):.4f}\")\n",
    "        print(f\"  Top 25% mean actual: {np.mean(metrics['top25_actual']):.4f}\")\n",
    "    \n",
    "    return models, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "models_all = {}\n",
    "\n",
    "for key in [\"up_low\", \"up_mid\", \"up_high\", \"down_low\", \"down_mid\", \"down_high\"]:\n",
    "    models, metrics = train_ensemble_model(\n",
    "        X_data[key], y_data[key],\n",
    "        f\"{key.upper()}\",\n",
    "        FEATURE_COLUMNS,\n",
    "        best_params=best_params,\n",
    "    )\n",
    "    models_all[key] = models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_feature_importance(models_dict: Dict, feature_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Average feature importance across all models\"\"\"\n",
    "    all_importances = []\n",
    "    \n",
    "    for key, models in models_dict.items():\n",
    "        for model in models:\n",
    "            imp = pd.DataFrame({\n",
    "                \"feature\": feature_cols,\n",
    "                \"importance\": model.feature_importances_,\n",
    "                \"model\": key\n",
    "            })\n",
    "            all_importances.append(imp)\n",
    "    \n",
    "    if not all_importances:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df_imp = pd.concat(all_importances)\n",
    "    return df_imp.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False)\n",
    "\n",
    "fi = get_ensemble_feature_importance(models_all, FEATURE_COLUMNS)\n",
    "print(\"Top 20 Features:\")\n",
    "print(fi.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models_v3\", exist_ok=True)\n",
    "\n",
    "# Save all models (single horizon since triple-barrier is time-adaptive)\n",
    "for key, models in models_all.items():\n",
    "    if models:\n",
    "        path = f\"models_v3/models_{key}.pkl\"\n",
    "        joblib.dump(models, path)\n",
    "        print(f\"Saved {path}\")\n",
    "\n",
    "# Save best hyperparameters\n",
    "if best_params:\n",
    "    with open(\"models_v3/best_params.json\", \"w\") as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "    print(\"Saved best_params.json\")\n",
    "\n",
    "print(\"\\nDone! Models saved to models_v3/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**V3 Enhancements:**\n",
    "\n",
    "1. **Cross-Sectional Features**: 5 rank-based features comparing each symbol to others\n",
    "2. **Triple-Barrier Labels**: Realistic TP (2x ATR), SL (1x ATR), Time (5 min) exits\n",
    "3. **Optuna Optimization**: 25 trials to find best hyperparameters\n",
    "4. **21 Days Data**: 50% more training data than V2\n",
    "5. **Memory Optimized**: Per-symbol processing, float32 throughout\n",
    "\n",
    "**To use:**\n",
    "1. Copy `models_v3/*.pkl` to production\n",
    "2. Update `feature_columns_v3.json`\n",
    "3. Update predictor to compute cross-sectional features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
